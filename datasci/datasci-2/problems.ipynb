{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a95aea7-3c78-4dc9-8a46-ac3ac0827db8",
   "metadata": {},
   "source": [
    "# Data science in Python I. - Problems in data science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6ee0f-bd5b-4572-8b78-606359d1c2cd",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "Data science primarily deals with datasets (surprise-surprise). There are a number of fundamental terms this field of science is built upon. It is undeniably necessary to be familiar with them before venturing any further into this field. Below you find a short list of the most important terms. A short description is also attached to all of them.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:16px;\">\n",
    "<li><b>Dataset</b> (<i>hu: adathalmaz</i>): An ensemble of datapoints. Denoted by upper case <code>X</code> by convention. Datasets can be (and most of the time is) multidimensional, which means the <code>x</code> (lower case) datapoints consist of more, than one components. In this case, datapoints can be considered to be \"vectors\", or at least a list of continuous/discrete/non-numeric values. It is also a common convention that in a table, rows denote the individual datapoints, while columns denote the different dimensions/components of datapoints.</li>\n",
    "\n",
    "<li><b>Labels</b> (<i>hu: címke</i>): Some datasets are not solely consists of the datapoints themeselves, but corresponding <b>labels</b> too. In normal cicumstances, every <code>x</code> datapoint has a corresponding <code>y</code> value. The list of labels are denoted by lower case <code>y</code> by convention.</li>\n",
    "\n",
    "<li><b>Features</b> (<i>no hu translation</i>): Another name for the different dimensions of an <code>X</code> dataset. This is the term that is primarily used for dataset dimensions in data science. In practice, most of the \"features\" represent an actual, measurable quantity.</li>\n",
    "\n",
    "<li><b>Class</b> (<i>hu: osztály</i>): In classifications problems, labels are discrete, which represent that every datapoint can be \"classified\" into a specific subset of the dataset. The interpretation of subsets can be arbitrary. They could simply represent \"bins\" or \"intervals\", in which the labels are \"binned to\" by value. Or they could be more meaningful. Eg. if the datapoints are images, labels could represent, whether there is a dog or a cat on the image.</li>\n",
    "\n",
    "<li><b>Model</b> (<i>same in magyar with double 'L'</i>): A \"model\" in data science has obviously a very similar meaning as in other fields of science. It means to represent some underlying connection between datapoins or between datapoints and labels in a dataset. In the context of modern data science, \"model\" represent an arbitrary mathematical operator or sequence of operators, that maps the <code>X</code> dataset to corresponding <code>y</code> values.</li>\n",
    "\n",
    "<li><b>Training/Learning</b> (<i>hu: tanulás/tanítás</i>): This is just a fancy and generalized way of saying \"fitting data on a model\". In numerous cases \"fitting data\" is not just a simple curve fitting, but a much more complex process that is harder to interpret. Also most machine learning methods work in a way, where they're optimizing model parameters during an iterative process. This can be well described by the terms \"training\" and \"learning\". Models are essentially \"trained\" over iterations, as they're \"learning\" the underlying correlation in the dataset.</li>\n",
    "\n",
    "<li><b>Supervised vs. unsupervised learning</b> (<i>hu: felügyelt-/felügyelet nélküli tanítás/tanulás</i>): It means whether we're using data with or without labels. If labels are attached to a dataset during training/learning, we're speaking about <b>supervised learning</b>, while if no labels are attached to our dataset, we're speaking about <b>unsupervised learning</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a505bf-1dd2-4ae2-9ea3-ad06651c4c10",
   "metadata": {},
   "source": [
    "# Types of problems\n",
    "\n",
    "Overwhelming majority of the problems in data science can be classified into 3 groups: regression, classification and clustering.\n",
    "\n",
    "<img width=\"700px\" src=\"./images/three-pillars.png\" style=\"display:block; margin:auto;\"/>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 1. The three pillars of data science</b>\n",
    "</p>\n",
    "<p style=\"text-align:center; font-size:12px;\">\n",
    "  <b>Source: <a href=\"https://www.researchgate.net/figure/The-three-pillars-of-learning-in-data-science-clustering-flat-or-hierarchical_fig1_314626729\">https://www.researchgate.net/figure/The-three-pillars-of-learning-in-data-science-clustering-flat-or-hierarchical_fig1_314626729</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af7da0-7018-4ad5-80f8-0d78184f9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn, tensorflow, torch, etc.\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification, \\\n",
    "                             make_blobs, make_moons, make_circles\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seaborn with custom settings\n",
    "# Facecolor values from S. Conradi @S_Conradi/@profConradi\n",
    "custom_settings = {\n",
    "    'figure.facecolor': '#f4f0e8',\n",
    "    'axes.facecolor': '#f4f0e8',\n",
    "    'axes.edgecolor': '0.7',\n",
    "    'axes.linewidth' : '2',\n",
    "    'grid.color': '0.7',\n",
    "    'grid.linestyle': 'none',\n",
    "    'grid.alpha': 0.6,\n",
    "}\n",
    "sns.set_theme(palette=sns.color_palette('deep', as_cmap=False),\n",
    "              rc=custom_settings)\n",
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc8a68-b5fb-43ce-809b-9304b30fbf66",
   "metadata": {},
   "source": [
    "## 1. Regression\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Continuous value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d75fa8-69af-4e8f-9431-eb79778a7670",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15749d56-f3ed-42e9-8492-f0ad3fbd90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    random_state=57\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b43c53-5a3c-4ad9-9b24-04440fe86f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe306c-71df-4ea3-8db2-3395de79c90b",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327771d-5fb3-484c-8ca4-6914937e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5), dpi=120)\n",
    "\n",
    "ax.plot(y, color='indianred', lw=2)\n",
    "\n",
    "ax.set_title('$y_{\\\\text{values}}$',\n",
    "             fontsize=30, fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7aa5c-4781-4dd6-a11d-5a1085327f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18feb50b-a2e3-456f-a7a9-b2a6f170b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a6879-2ed3-4078-a64b-528873845b57",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Discrete value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada391f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d23e2-91c1-46a7-9939-5b15deace323",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_redundant=0,\n",
    "    n_classes=3\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db458744-5757-4cf3-b562-b8452aee407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c994ee9-5c1d-4bbb-9815-0dd9e1d33f6c",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(y.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f241c-2b00-4c0c-ab45-1f5ae9d0ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "\n",
    "ax.barh(*np.unique(y, return_counts=True), height=0.7,\n",
    "        color=cm.tab10(np.unique(y)))\n",
    "\n",
    "ax.set_yticks(np.unique(y))\n",
    "ax.set_yticklabels(np.unique(y))\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f1dd1-80cf-48ef-863a-8281eeebb26b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d936b22-90b6-460c-bc88-3bacbb0036fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda427b-b9f1-4a5e-bb09-6e061e505d1d",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Only data -> Model -> Discrete value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f0d88-df7f-4d1c-83e7-1509cc669b61",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1112f4-f711-4ede-a570-d4dc619d113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=N,    # Number of points in the dataset (number of rows)\n",
    "    n_features=2,   # Dimension of the dataset (number of columns)\n",
    "    centers=3,      # Number of blobs to create\n",
    "    cluster_std=[1.0, 2.5, 0.5],\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of circles\n",
    "Xc, yc = make_circles(\n",
    "    n_samples=N,\n",
    "    noise=0.05,\n",
    "    factor=0.6,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of moons\n",
    "Xm, ym = make_moons(\n",
    "    n_samples=N,\n",
    "    noise=0.05,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce244944",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Blobs:   ' + ' '.join(yb.astype(str)))\n",
    "print('Circles: ' + ' '.join(yc.astype(str)))\n",
    "print('Moons:   ' + ' '.join(ym.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7919f21",
   "metadata": {},
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc8e58-4287-4861-9ae2-3e3f534e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize them\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(8*nc, 8*nr))\n",
    "\n",
    "Xi = (Xb, Xc, Xm)\n",
    "yi = (yb, yc, ym)\n",
    "for X, y, ax in zip(Xi, yi, axes.flat):\n",
    "\n",
    "    X = X - np.mean(X)\n",
    "    ax.scatter(*X.T, c=cm.viridis(y/np.max(y)), alpha=0.6)\n",
    "\n",
    "    lim = 1.1 * np.max(np.abs(X))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562d81d-e078-4a32-acf6-57d1feb1e594",
   "metadata": {},
   "source": [
    "### Let's have a look at the first one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b96b5-8dc7-43c2-9e05-29ee45ce7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=1000,  # Number of points in the dataset (number of rows)\n",
    "    n_features=10,   # Dimension of the dataset (number of columns)\n",
    "    centers=3,       # Number of blobs to create\n",
    "    cluster_std=1.5,\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "Xb = pd.DataFrame(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53b107-3fec-4319-b2cb-da9283229f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a64b0f-26b7-44b5-b377-3072248fda52",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Although we assign some groundtruth labels to our data, in case of clustering, we do not use them during the training process. In a real life scenario, we do not have any labels for our data, only the data itself. To test the robustness and accuracy of our model, however, we can obviously generate data sets with already known labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876c3cc-59f2-4122-bc6e-4ff34aef78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(yb.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce870c-0e95-4e8b-9616-f51f3f4da15d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee33f3-c14c-4469-808b-07b791cd349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(Xb[i], yb,\n",
    "               color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68648b-380d-4463-ba53-849beb0519a0",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469b3ac-4055-439b-839e-2c6cdfebd848",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "If we consider images as **datapoints** (**rows**) in a dataset, then pixels of images can be considered as individual *features* (*columns*) of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf9b01-2beb-458c-84ef-fc946eb8cc3b",
   "metadata": {},
   "source": [
    "#### Sloan Digital Sky Surve (SDSS) galaxy tiles and corresponding redshift values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfe0c5",
   "metadata": {},
   "source": [
    "**Install SciScript using `conda`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f010eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sciserver/SciScript-Python.git ./tmp/SciScript-Python\n",
    "# %cd ./tmp/SciScript-Python/py3\n",
    "# !python -m build\n",
    "# %pip install dist/*.whl\n",
    "# %cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3d43d",
   "metadata": {},
   "source": [
    "**Install SciScript using `pip`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99813d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sciserver/SciScript-Python.git -p ./tmp/SciScript-Python\n",
    "# %cd ./tmp/SciScript-Python/py3\n",
    "# !python -m build\n",
    "# %pip install dist/*.whl\n",
    "# %cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.sdss import SDSS\n",
    "\n",
    "from astropy import coordinates as coords\n",
    "from astropy.visualization import ImageNormalize, ZScaleInterval\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "from SciServer import SkyServer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd17b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT TOP 15\n",
    "    g.ra, g.dec, g.PetroRad_r, s.z\n",
    "FROM Galaxy g\n",
    "    JOIN\n",
    "        SpecObj s ON s.specObjID = g.specObjID\n",
    "WHERE\n",
    "    g.clean=1\n",
    "    AND g.PetroRad_r BETWEEN 20 AND 32\n",
    "    AND s.z BETWEEN 0.02 AND 0.05\n",
    "'''\n",
    "data = SDSS.query_sql(query, data_release=17).to_pandas()\n",
    "co = coords.SkyCoord(ra=data['ra'], dec=data['dec'], unit='deg', frame='icrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3495ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for ra, dec in tqdm(zip(data['ra'], data['dec'])):\n",
    "    img = SkyServer.getJpegImgCutout(ra=ra, dec=dec,\n",
    "                                     width=96, height=96, scale=0.7, opt=\"\")\n",
    "    imgs.append(img)\n",
    "imgs = np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5f0a1-53ed-4697-b209-be69b6f3b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = np.random.randint(len(imgs))\n",
    "img = imgs[di]\n",
    "\n",
    "print(f\"{img.shape = }\")\n",
    "print(f\"num of pixels = {img.size}\")\n",
    "print(f\"Redshift is z = {data.z[di]}\")\n",
    "\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*6, nr*6), dpi=120)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "# Plot all 3 color channels\n",
    "ax = axes[0]\n",
    "ax.imshow(img[..., 0], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Red Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.imshow(img[..., 1], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Green Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.imshow(img[..., 2], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Blue Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c29fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(img[..., 1])  # Printing one of the color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89337be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2), dpi=200)\n",
    "\n",
    "ax.plot(img.flatten(), color='indianred', lw=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim(0, img.size)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36370709-9e22-48d0-bdc2-8921bf9d074b",
   "metadata": {},
   "source": [
    "#### Now imagine a whole data set of images like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55de5a4-baf3-471d-a6ae-787642db7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 3, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*4, nr*4), dpi=120,\n",
    "                         facecolor='black')\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis(False)\n",
    "    \n",
    "# Plot cutouts\n",
    "for ax, img in zip(axes, imgs):\n",
    "    ax.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(imgs[..., 0].reshape((-1, img.shape[0] * img.shape[1])))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83e2a3",
   "metadata": {},
   "source": [
    "You can even plot this flattened data set, but with so few rows, it is not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20), dpi=600)\n",
    "fig.patch.set_visible(False)  # remove figure background\n",
    "\n",
    "ax.axis(False)\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(X.values, interpolation='none', cmap='Greys_r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e4916",
   "metadata": {},
   "source": [
    "### Mixed dataset\n",
    "\n",
    "Data of 891 Titanic passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad134d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"http://patbaa.web.elte.hu/physdm/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030927",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 24), facecolor='black')\n",
    "\n",
    "# Determine the image extent and axis limits for dear Mr. Matplotlib\n",
    "x_lim = (0, X.values.shape[0]-1)\n",
    "y_lim = [-0.5, X.values.shape[1]-0.5]\n",
    "\n",
    "ax.imshow(X.isna().values.T,\n",
    "          extent=(x_lim[0], x_lim[-1], y_lim[0], y_lim[-1]),\n",
    "          aspect=10, cmap=\"Greys\", interpolation='none')\n",
    "\n",
    "# Y-AXIS FORMATTING\n",
    "ax.set_yticks(range(X.columns.size))\n",
    "ax.set_yticklabels(X.columns[::-1], ha='right')\n",
    "ax.tick_params(axis='both', which='major',\n",
    "               labelsize=12, pad=10, colors='white')\n",
    "\n",
    "ax.grid(True, axis='y', ls='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234393b",
   "metadata": {},
   "source": [
    "**This dataset needs some preprocessing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f0136",
   "metadata": {},
   "source": [
    "### A completely different type of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = \"MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEK\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19003b",
   "metadata": {},
   "source": [
    "<img src=\"./images/alphafold.png\"/></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697363e",
   "metadata": {},
   "source": [
    "# How to approach and handle a problem in data science?\n",
    "\n",
    "Most of the problems should be approached and treated similarly by following these simple steps:\n",
    "- Step 1.: Preprocess the dataset for analysis\n",
    "- Step 2.: Find, tune and fit a model or models on the preprocessed dataset\n",
    "- Step 3.: Make predictions using the trained model and evaluate and interpret the results\n",
    "\n",
    "<img src=\"./images/pipeline-full.png\"/></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672118c6",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "A lot of beginner machine learning/data science guide for specific datasets will tell you to work with the data in a very specific way without actually telling you **why** should you do it **that** way? Why *scaling* the data is necessary? Why should you use *hot encoding*? What else can be done about missing data entries besides simply dropping them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e5f7",
   "metadata": {},
   "source": [
    "## 1.0. Every data preprocessing starts with data exploration\n",
    "\n",
    "### Why? Because looking at the data could be extremely insightful...\n",
    "\n",
    "See this example at https://en.wikipedia.org/wiki/Anscombe's_quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sns.load_dataset(\"anscombe\")  # Load Anscombe's quartet from seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976210ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 4\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(5*nc, 5*nr), dpi=120)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for di, ax in zip(X.dataset.unique(), axes.flat):\n",
    "    Xs = X.query(f\"dataset == '{di}'\")['x']\n",
    "    ys = X.query(f\"dataset == '{di}'\")['y']\n",
    "    ax.scatter(Xs, ys, s=15**2)\n",
    "    xi = np.linspace(-10, 25)\n",
    "    yi = 1/2 * xi + 3.0\n",
    "    ax.plot(xi, yi, color='tab:red', lw=5, alpha=0.7)\n",
    "    ax.set_xlim(0.8 * np.min(Xs), 1.1 * np.max(Xs))\n",
    "    ax.set_ylim(0.8 * np.min(ys), 1.1 * np.max(ys))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860af3",
   "metadata": {},
   "source": [
    "#### 1.0.1. Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966970ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"http://patbaa.web.elte.hu/physdm/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827a5a2",
   "metadata": {},
   "source": [
    "### 1.0.2. Exploring missing data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e681cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 24), facecolor='black')\n",
    "\n",
    "# Determine the image extent and axis limits for dear Mr. Matplotlib\n",
    "x_lim = (0, X.values.shape[0]-1)\n",
    "y_lim = [-0.5, X.values.shape[1]-0.5]\n",
    "\n",
    "ax.imshow(X.isna().values.T,\n",
    "          extent=(x_lim[0], x_lim[-1], y_lim[0], y_lim[-1]),\n",
    "          aspect=10, cmap=\"Greys\", interpolation='none')\n",
    "\n",
    "# Y-AXIS FORMATTING\n",
    "ax.set_yticks(range(X.columns.size))\n",
    "ax.set_yticklabels(X.columns[::-1], ha='right')\n",
    "ax.tick_params(axis='both', which='major',\n",
    "               labelsize=15, pad=10, colors='white')\n",
    "\n",
    "ax.grid(True, axis='y', ls='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a70906",
   "metadata": {},
   "source": [
    "### 1.0.3. Exploring datatypes in the dataset\n",
    "\n",
    "Object? Int? Float? Other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `dtypes` variable of a pandas DataFrame object stores the datatypes\n",
    "# of the columns in a specific DataFrame object\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d383c",
   "metadata": {},
   "source": [
    "### 1.0.4. Exploring distribution of feature values\n",
    "\n",
    "Explore a randomly generated classification dataset with 2 distinct classes and 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=8,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc, nr = 4, 2\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(6*nc, 5*nr))\n",
    "\n",
    "mask = np.bool_(y)\n",
    "data = [X[mask], X[~mask]]\n",
    "cmap = [cm.Reds, cm.Blues]\n",
    "labl = ['Class 0', 'Class 1']\n",
    "for d, c, l in zip(data, cmap, labl):\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "        # Convention for plotting numpy.histogram results\n",
    "        hist, bins = np.histogram(d.values[:, i], bins=20, density=True)\n",
    "        width = 0.8 * (bins[1] - bins[0])\n",
    "        center = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.bar(center, hist, width=width, label=l,\n",
    "               color=c(0.6), alpha=0.6)\n",
    "\n",
    "        ax.set_title(f\"Feature {i}\", fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='upper left', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657e051",
   "metadata": {},
   "source": [
    "### 1.0.5. Exploring the correlation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=100,    # Number of points in the data set\n",
    "    n_features=6,     # Number of features in the data set\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8395985",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2333a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    X,\n",
    "    kind='scatter',\n",
    "    diag_kind='kde'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b1bb1",
   "metadata": {},
   "source": [
    "## 1.1. Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d0e5c",
   "metadata": {},
   "source": [
    "### 1.1.1. Deleting rows or columns with too much NaN values\n",
    "\n",
    "- Rows or columns with too many missing values cannot be filled up in a meaningful way that is representative of the underlying distribution of the data\n",
    "- It is better to simply drop rows or columns with too many missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a087bb5",
   "metadata": {},
   "source": [
    "### 1.1.2. Filling empty entries with values\n",
    "\n",
    "- Filling NaN entries with mean of existing values\n",
    "- Filling NaN entries with values sampled from the distribution of existing ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4030e9",
   "metadata": {},
   "source": [
    "## 1.2. Handling non-numeric data\n",
    "\n",
    "- E.g. utilize label encoding and one-hot encoding\n",
    "- Or do nothing with them in case of using various tree ensemble methods (e.g. TabNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186260eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "categories = ['Dog', 'Cat', 'Raccoon', 'Bear']\n",
    "feat = np.random.choice(categories, size=100, replace=True)\n",
    "X[5] = pd.Series(feat)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c4851",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "\n",
    "Used, when the categorical feature is ordinal, i.e. the values have a natural order.\n",
    "- E.g. \"rite\", \"cum laude\", \"summa cum laude\" can be encoded to 0, 1, 2\n",
    "- E.g. \"small\", \"medium\", \"large\", \"extra large\" can be encoded to 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Xenc = le.fit_transform(X.iloc[:, 5])\n",
    "pd.concat([X.iloc[:, :-1], pd.Series(Xenc, name=X.columns[5])], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d5943",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Used, when the categorical feature is nominal, i.e. the values do not have a natural order.\n",
    "- E.g. \"red\", \"green\", \"blue\" can be encoded to 3 binary features\n",
    "- E.g. \"bear\", \"cat\", \"dog\", \"raccoon\" can be encoded to 4 binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1.\n",
    "encoder = OneHotEncoder(categories=[categories], sparse_output=False)\n",
    "Xenc = encoder.fit_transform(X.iloc[:, 5].values.reshape(-1, 1))\n",
    "Xenc = pd.DataFrame(Xenc.astype(int), columns=categories)\n",
    "pd.concat((X.iloc[:, :-1], Xenc), axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de227c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2.\n",
    "Xenc = pd.get_dummies(X.iloc[:, -1], dtype=int)\n",
    "pd.concat([X.iloc[:, :-1], Xenc], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b2a40",
   "metadata": {},
   "source": [
    "### Why can (and should) we use these two methods in these two different cases?\n",
    "\n",
    "In the case of **one-hot encoding**, we prevent the model from learning false correlations between different values of a categorical feature. If we instead assigned ascending integer values to these categories, we would be mapping them to a **discrete, ordered scale**. Statistical models would then interpret this as if a meaningful order exists&mdash;for example, that if `0` is assigned to \"small\" and `1` to \"medium\", then \"small\" $<$ \"medium\", which is **correct** when the categories are **ordinal**.\n",
    "\n",
    "However, this logic falls apart when applied to **nominal** categories such as \"red\", \"green\", and \"blue\". Assigning `1` to \"green\" and `2` to \"blue\" would incorrectly suggest to the model that \"green\" $<$ \"blue\"&mdash;which is not only **nonsensical**, but potentially **harmful** to model performance (unless you are a color zealot, in which case that is a separate issue and we will NOT ask your opinion about this question).\n",
    "\n",
    "That is why **one-hot encoding** is used for nominal features: it treats each category as **independent**, avoiding any unintended assumptions about order or distance between values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f5dc7",
   "metadata": {},
   "source": [
    "## 1.3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a1d7a",
   "metadata": {},
   "source": [
    "### 1.3.1 Example using an SDSS plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f971d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = SDSS.get_images(coordinates=co)\n",
    "X = pd.DataFrame(imgs[0][0].data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6756d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(X.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1acdd6",
   "metadata": {},
   "source": [
    "### Machine learning algorithms don't like values all over the scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44035556",
   "metadata": {},
   "source": [
    "#### StandardScaler\n",
    "\n",
    "Scales each individual **feature (column)** to have a mean of $0$ and a standard deviation of $1$.\n",
    "\n",
    "$$\n",
    "    x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of each feature. This is useful when features have different scales or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = StandardScaler().fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cad3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(Xs.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ea67d",
   "metadata": {},
   "source": [
    "#### Normalizer  \n",
    "Scales each individual **sample (row)** to have unit norm, meaning the entire row vector has length $1$. This is often useful when the direction of the data matters more than its magnitude. Normalizer uses either L1 or L2 norm to scale the data.\n",
    "\n",
    "- **L2 norm** (Euclidean norm): scales the vector so that\n",
    "  $$\n",
    "      \\| \\mathbf{x} \\|_{2} = \\sqrt{x_{1}^{2} + x_{2}^{2} + \\cdots + x_{n}^{2}} = 1\\,.\n",
    "  $$\n",
    "  Each element is then divided by the L2 norm of the vector:\n",
    "  $$\n",
    "      x'_i = \\frac{x_{i}}{\\| \\mathbf{x} \\|_2}\n",
    "  $$\n",
    "\n",
    "- **L1 norm** (Manhattan norm): scales the vector so that\n",
    "  $$\n",
    "      \\| \\mathbf{x} \\|_{1} = |x_{1}| + |x_{2}| + \\cdots + |x_{n}| = 1\\,.\n",
    "  $$\n",
    "  Each element is then divided by the L1 norm of the vector:\n",
    "  $$\n",
    "      x'_i = \\frac{x_{i}}{\\| \\mathbf{x} \\|_{1}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95affb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn = Normalizer(norm='l2').fit_transform(X)\n",
    "Xn = pd.DataFrame(Xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b53935",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(Xn.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03938d05",
   "metadata": {},
   "source": [
    "#### Images themselves need a different type of normalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf24050",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*10, nr*10))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.025,\n",
    "                    left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "ax = axes[0]\n",
    "ax.axis(False)\n",
    "ax.imshow(Xs.values, origin='lower', cmap='gray')\n",
    "ax.text(0.025, 0.975, 'Scaled Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.axis(False)\n",
    "ax.imshow(Xn.values, origin='lower', cmap='gray')\n",
    "ax.text(0.025, 0.975, 'Normalized Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axis(False)\n",
    "norm = ImageNormalize(X.values, interval=ZScaleInterval())\n",
    "ax.imshow(X.values, origin='lower', cmap='gray', norm=norm)\n",
    "ax.text(0.025, 0.975, 'IRAF\\'s zscale', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d748",
   "metadata": {},
   "source": [
    "### 1.3.2 Example using a 2D map from the CAMELS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20352c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a larger image file (~100 Mb), it could take a while to download,\n",
    "# depending on your internet connection and the server load!\n",
    "#\n",
    "# URL might change in the future, search for the latest version\n",
    "# at https://camels-multifield-dataset.readthedocs.io/en/latest/access.html\n",
    "url = 'https://users.flatironinstitute.org/~fvillaescusa/priv/DEPnzxoWlaTQ6CjrXqsm0vYi8L7Jy/CMD/2D_maps/data/IllustrisTNG/Maps_Mtot_IllustrisTNG_CV_z=0.00.npy'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Immediately throws error if the download failed\n",
    "\n",
    "X = np.load(io.BytesIO(response.content))\n",
    "X = pd.DataFrame(X.reshape((-1, 256**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*6, nr*6), dpi=120)\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.025,\n",
    "                    left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "\n",
    "img = X.iloc[X.sum(axis=1).argmax()].values.reshape((256, 256))\n",
    "cmap = 'magma'\n",
    "\n",
    "ax = axes[0]\n",
    "ax.axis(False)\n",
    "ax.imshow(img, cmap=cmap)\n",
    "ax.text(0.025, 0.975, 'Raw Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.axis(False)\n",
    "ax.imshow(np.log10(img), cmap=cmap)\n",
    "ax.text(0.025, 0.975, '$\\\\log_{10}$', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axis(False)\n",
    "norm = ImageNormalize(img, interval=ZScaleInterval())\n",
    "ax.imshow(img, cmap=cmap, norm=norm)\n",
    "ax.text(0.025, 0.975, 'IRAF\\'s zscale', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798e652-7d34-4e01-813d-3fda9bc566e4",
   "metadata": {},
   "source": [
    "## 1.4. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5caab-41ec-47bb-8209-d75d755e0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62813bda-46ec-4e03-be46-ac1feb339373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...in the next section..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

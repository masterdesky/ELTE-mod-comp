{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a95aea7-3c78-4dc9-8a46-ac3ac0827db8",
   "metadata": {},
   "source": [
    "# Classical problems in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6ee0f-bd5b-4572-8b78-606359d1c2cd",
   "metadata": {},
   "source": [
    "The terms \"data science\", \"machine learning\" (ML) and unfortunately, \"AI\" as well are nowadays often used interchangeably, however, they describe related, but entirely different concepts. Data science is a broader umbrella that encompasses machine learning as one of its main methodology, but it also deeply intersects with other areas, most importantly applied mathematics and statistics. Additionally, data science also includes essential practices of managing and understanding data, such as the myriads of data preprocessing and visualization techniques. On the other hand, the field of machine learning could be considered to be an *approach* that involves all&mdash;even developing&mdash;techniques to automatically create mathematical models from data. These models are capable of making predictions or decisions based on patterns recognized in the data, without needing explicit instructions for each specific decision. Contrary to the prior two, \"AI\" is mostly just a marketing term. Nowadays, everything is dubbed as \"AI\" if it involves some kind of automation or decision-making process.\n",
    "\n",
    "This article deals with machine learning in particular. It sets out to explore the fundamental problems that ML aims to solve, and to provide an overview of how these problems are approached in practice. Essentially, I try to provide a brief introduction to machine learning within the context of data science.\n",
    "\n",
    "There are a number of fundamental terms that the field of data science is built upon (which is partly just ML jargon). Undoubtedly, it is necessary to be familiar with them before venturing any deeper into this genuinely vast field of research. Below, you find a list of the most important terms along with some short descriptions. This list is not comprehensive; it is only meant to give a brief overview of the most important concepts that fit into the scope of this introductory notebook.\n",
    "\n",
    "### Glossary\n",
    "- **Data set** (_hu: adatsor/adathalmaz_): A data set is an ensemble of data points organized into a table-like structure, where each row represents a single data point and columns correspond to the various attributes of these points. These attributes may include both \"independent variables\", and&mdash;if applicable&mdash;\"dependent variables\" known as labels; both of which could contain either numerical, categorical (non-numerical), or mixed values. Data sets are typically multidimensional, meaning that each data point consists of multiple components, making them capable of representing complex information in array or vector forms.\n",
    "- **Features** (_no good hu translation_): Independent variables in a table-like data set are often referred to as \"features\" in data science. The set of features present in a data set is commonly denoted by uppercase $X$. Features represent measurable attributes or characteristics of data points, and they provide the input variables for machine learning models. A common goal in general is to find, measure and use attributes in a data set that are applicable to most data points. This validates the naming scheme to call these attributes \"features\" (or descriptors/measurables) of a data set.\n",
    "- **Labels** (_hu: címkék_): Labels are the dependent variables in a data set, and the set of labels present is commonly denoted by lowercase $y$. Labels can be single-dimensional or multi-dimensional, and like features, they may be numerical, categorical, or a mix of both. They can also vary as continuous or discrete values, depending on the nature of the prediction task. When labels represent the outcomes or categories that a model aims to predict based on the independent variables or features, we refer to the task as a \"supervised learning\" problem. In contrast, when labels are absent, the task is referred to as \"unsupervised learning\".\n",
    "- **Class** (_hu: osztály_): When labels take categorical values, we refer to them as \"classes\". The name represent that every data point can be \"classified\" into different groups. The meaning of these groups can be arbitrary and it always depends on the specific task and data set. They could simply represent bins, intervals or clusters that data points fit into. But for instance, in a data set composed of images, classes may denote categories like \"dog\" or \"cat\". Although the terms are related, \"class\" should not be confused with \"classification\" or \"clustering\". While all involve grouping based on similarities, classification specifically predicts the class for new data points based on supervised learning, and clustering groups data points into subsets without predefined labels, making it an unsupervised approach.\n",
    "- **Model** (_same in magyar with 'll'_): A model in ML has an almost identical meaning as in the general, scientific sense of the word. It is a simplified representation of some underlying connection between either data points and corresponding labels or between the individual data points themselves. A model can be a simple mathematical function, a complex neural network, or anything in between. The main characteristic of models in ML was already mentioned in the introduction: they are created automatically from data, contrary to the traditional scientific models that are created by humans, based on prior knowledge and assumptions.\n",
    "- **Parameters** (_hu: paraméterek_): Parameters are the numerical values that define the structure of a model. Their number is solely determined by the complexity of the model and the number of features in the data set. E.g. in a 1D line fitting model, the parameters are the slope and the intercept of the line. In a neural network, the parameters are the weights and biases of the neurons and their total number could easily be in the millions or more.\n",
    "- **Training/Learning** (_hu: tanítás/tanulás_): This term refers to the process by which machine learning models are created. Analogous to the term \"fitting\" in statistics, training involves the use of some algorithm(s) to find the best mathematical model that describes the relationships between different variables in a given data set. Training can range from a simple linear regression to more complex operations involving sophisticated mathematical models and multiple parameters. However, at its core, the process always involves a series of optimization steps where the model parameters are iteratively adjusted to maximize some predefined accuracy metric. The terms \"training\" and \"learning\" reflects this iterative optimization aspect, as the model incrementally improves its predictions by uncovering and adapting (or \"fitting\") to the underlying patterns/correlations in the data.\n",
    "- **Supervised/Unsupervised learning** (_hu: felügyelt-/felügyelet nélküli tanítás/tanulás_): This term refers to the two main approaches in machine learning. In supervised learning, the model is trained on a labeled data set, meaning that each data point has a corresponding label or target value. The goal is to learn a mapping from the input features to the output labels. In contrast, unsupervised learning involves training on an unlabeled data set, where the model must discover patterns or structures in the data without any explicit guidance. This can include clustering similar data points together or reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a505bf-1dd2-4ae2-9ea3-ad06651c4c10",
   "metadata": {},
   "source": [
    "# Types of problems\n",
    "\n",
    "Overwhelming majority of the problems in data science can be classified into 3 groups: regression, classification and clustering.\n",
    "\n",
    "<img width=\"700px\" src=\"./images/three-pillars.png\" style=\"display:block; margin:auto;\"/>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 1. The three pillars of data science</b>\n",
    "</p>\n",
    "<p style=\"text-align:center; font-size:12px;\">\n",
    "  <b>Source: <a href=\"https://www.researchgate.net/figure/The-three-pillars-of-learning-in-data-science-clustering-flat-or-hierarchical_fig1_314626729\">https://www.researchgate.net/figure/The-three-pillars-of-learning-in-data-science-clustering-flat-or-hierarchical_fig1_314626729</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af7da0-7018-4ad5-80f8-0d78184f9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn, tensorflow, torch, etc.\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification, \\\n",
    "                             make_blobs, make_moons, make_circles\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seaborn with custom settings\n",
    "# Facecolor values from S. Conradi @S_Conradi/@profConradi\n",
    "custom_settings = {\n",
    "    'figure.facecolor': '#f4f0e8',\n",
    "    'axes.facecolor': '#f4f0e8',\n",
    "    'axes.edgecolor': '0.7',\n",
    "    'axes.linewidth' : '2',\n",
    "    'grid.color': '0.7',\n",
    "    'grid.linestyle': 'none',\n",
    "    'grid.alpha': 0.6,\n",
    "}\n",
    "sns.set_theme(palette=sns.color_palette('deep', as_cmap=False),\n",
    "              rc=custom_settings)\n",
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc8a68-b5fb-43ce-809b-9304b30fbf66",
   "metadata": {},
   "source": [
    "## 1. Regression\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Continuous value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d75fa8-69af-4e8f-9431-eb79778a7670",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15749d56-f3ed-42e9-8492-f0ad3fbd90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    random_state=57\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b43c53-5a3c-4ad9-9b24-04440fe86f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe306c-71df-4ea3-8db2-3395de79c90b",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327771d-5fb3-484c-8ca4-6914937e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5), dpi=120)\n",
    "\n",
    "ax.plot(y, color='indianred', lw=2)\n",
    "\n",
    "ax.set_title('$y_{\\\\text{values}}$',\n",
    "             fontsize=30, fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7aa5c-4781-4dd6-a11d-5a1085327f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18feb50b-a2e3-456f-a7a9-b2a6f170b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a6879-2ed3-4078-a64b-528873845b57",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Discrete value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada391f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d23e2-91c1-46a7-9939-5b15deace323",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_redundant=0,\n",
    "    n_classes=3\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db458744-5757-4cf3-b562-b8452aee407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c994ee9-5c1d-4bbb-9815-0dd9e1d33f6c",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(y.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f241c-2b00-4c0c-ab45-1f5ae9d0ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "\n",
    "ax.barh(*np.unique(y, return_counts=True), height=0.7,\n",
    "        color=cm.tab10(np.unique(y)))\n",
    "\n",
    "ax.set_yticks(np.unique(y))\n",
    "ax.set_yticklabels(np.unique(y))\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f1dd1-80cf-48ef-863a-8281eeebb26b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d936b22-90b6-460c-bc88-3bacbb0036fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda427b-b9f1-4a5e-bb09-6e061e505d1d",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Only data -> Model -> Discrete value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f0d88-df7f-4d1c-83e7-1509cc669b61",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1112f4-f711-4ede-a570-d4dc619d113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=N,    # Number of points in the dataset (number of rows)\n",
    "    n_features=2,   # Dimension of the dataset (number of columns)\n",
    "    centers=3,      # Number of blobs to create\n",
    "    cluster_std=[1.0, 2.5, 0.5],\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of circles\n",
    "Xc, yc = make_circles(\n",
    "    n_samples=N,\n",
    "    noise=0.05,\n",
    "    factor=0.6,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of moons\n",
    "Xm, ym = make_moons(\n",
    "    n_samples=N,\n",
    "    noise=0.05,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce244944",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "In case of a clustering problem, we do not actually have any labels, as we only trying to find spatial patterns across the data points. However, we can still assign labels to the data points based on the clusters they belong to. The labels assigned to the data points in this case are not true labels, but rather cluster identifiers that indicate which group each data point belongs to. If we generate a synthetic data set, we have the opportunity to pre-define clusters and assign labels to the data points accordingly. This allows us to evaluate the performance of our clustering algorithm by comparing the predicted labels with the true labels.\n",
    "\n",
    "However, in real-world applications, we typically do not have access to true labels for the data points, making it a more challenging task to assess the quality of the clustering results. In such cases, we often rely on other evaluation metrics, such as silhouette scores or Davies-Bouldin indices, to measure the quality of the clusters formed by the algorithm. These metrics assess the compactness and separation of the clusters without relying on true labels. These analysis techniques are not part of this notebook, but they are worth mentioning as they are important in the context of clustering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Blobs:   ' + ' '.join(yb.astype(str)))\n",
    "print('Circles: ' + ' '.join(yc.astype(str)))\n",
    "print('Moons:   ' + ' '.join(ym.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7919f21",
   "metadata": {},
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc8e58-4287-4861-9ae2-3e3f534e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize them\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(8*nc, 8*nr))\n",
    "\n",
    "Xi = (Xb, Xc, Xm)\n",
    "yi = (yb, yc, ym)\n",
    "for X, y, ax in zip(Xi, yi, axes.flat):\n",
    "\n",
    "    X = X - np.mean(X)\n",
    "    ax.scatter(*X.T, c=cm.viridis(y/np.max(y)), alpha=0.6)\n",
    "\n",
    "    lim = 1.1 * np.max(np.abs(X))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562d81d-e078-4a32-acf6-57d1feb1e594",
   "metadata": {},
   "source": [
    "### Let's have a look at the first one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b96b5-8dc7-43c2-9e05-29ee45ce7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=1000,  # Number of points in the dataset (number of rows)\n",
    "    n_features=10,   # Dimension of the dataset (number of columns)\n",
    "    centers=3,       # Number of blobs to create\n",
    "    cluster_std=1.5,\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "Xb = pd.DataFrame(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53b107-3fec-4319-b2cb-da9283229f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a64b0f-26b7-44b5-b377-3072248fda52",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Although we assign some groundtruth labels to our data, in case of clustering, we do not use them during the training process. In a real life scenario, we do not have any labels for our data, only the data itself. To test the robustness and accuracy of our model, however, we can obviously generate data sets with already known labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876c3cc-59f2-4122-bc6e-4ff34aef78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(yb.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce870c-0e95-4e8b-9616-f51f3f4da15d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data $\\times$ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee33f3-c14c-4469-808b-07b791cd349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(Xb[i], yb,\n",
    "               color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68648b-380d-4463-ba53-849beb0519a0",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469b3ac-4055-439b-839e-2c6cdfebd848",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "If we consider images as **datapoints** (**rows**) in a dataset, then pixels of images can be considered as individual *features* (*columns*) of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf9b01-2beb-458c-84ef-fc946eb8cc3b",
   "metadata": {},
   "source": [
    "#### Sloan Digital Sky Surve (SDSS) galaxy tiles and corresponding redshift values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3d43d",
   "metadata": {},
   "source": [
    "**Install SciScript**\n",
    "\n",
    "Uncomment all lines, when first installing SciScript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99813d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sciserver/SciScript-Python.git -p ./tmp/SciScript-Python\n",
    "# %cd ./tmp/SciScript-Python/py3\n",
    "# !python -m build\n",
    "# %pip install dist/*.whl\n",
    "# %cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.sdss import SDSS\n",
    "\n",
    "from astropy import coordinates as coords\n",
    "from astropy.visualization import ImageNormalize, ZScaleInterval\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "from SciServer import SkyServer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd17b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "SELECT TOP 15\n",
    "    g.ra, g.dec, g.PetroRad_r, s.z\n",
    "FROM Galaxy g\n",
    "    JOIN\n",
    "        SpecObj s ON s.specObjID = g.specObjID\n",
    "WHERE\n",
    "    g.clean=1\n",
    "    AND g.PetroRad_r BETWEEN 20 AND 32\n",
    "    AND s.z BETWEEN 0.02 AND 0.05\n",
    "'''\n",
    "data = SDSS.query_sql(query, data_release=17).to_pandas()\n",
    "co = coords.SkyCoord(ra=data['ra'], dec=data['dec'], unit='deg', frame='icrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3495ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for ra, dec in tqdm(zip(data['ra'], data['dec'])):\n",
    "    img = SkyServer.getJpegImgCutout(ra=ra, dec=dec,\n",
    "                                     width=96, height=96, scale=0.7, opt=\"\")\n",
    "    imgs.append(img)\n",
    "imgs = np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5f0a1-53ed-4697-b209-be69b6f3b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = np.random.randint(len(imgs))\n",
    "img = imgs[di]\n",
    "\n",
    "print(f\"{img.shape = }\")\n",
    "print(f\"num of pixels = {img.size}\")\n",
    "print(f\"Redshift is z = {data.z[di]}\")\n",
    "\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*6, nr*6), dpi=120)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "# Plot all 3 color channels\n",
    "ax = axes[0]\n",
    "ax.imshow(img[..., 0], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Red Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.imshow(img[..., 1], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Green Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.imshow(img[..., 2], interpolation='none', cmap='Greys_r')\n",
    "ax.text(0.025, 0.975, 'Blue Channel', color='white', fontweight='bold',\n",
    "        fontsize=20, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c29fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(img[..., 1])  # Printing one of the color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89337be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2), dpi=200)\n",
    "\n",
    "ax.plot(img.flatten(), color='indianred', lw=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim(0, img.size)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36370709-9e22-48d0-bdc2-8921bf9d074b",
   "metadata": {},
   "source": [
    "#### Now imagine a whole data set of images like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55de5a4-baf3-471d-a6ae-787642db7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 3, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*4, nr*4), dpi=120,\n",
    "                         facecolor='black')\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis(False)\n",
    "    \n",
    "# Plot cutouts\n",
    "for ax, img in zip(axes, imgs):\n",
    "    ax.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(imgs[..., 0].reshape((-1, img.shape[0] * img.shape[1])))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83e2a3",
   "metadata": {},
   "source": [
    "You can even plot this flattened data set, but with so few rows, it is not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20), dpi=600)\n",
    "fig.patch.set_visible(False)  # remove figure background\n",
    "\n",
    "ax.axis(False)\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(X.values, interpolation='none', cmap='Greys_r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e4916",
   "metadata": {},
   "source": [
    "### Mixed dataset\n",
    "\n",
    "Data of 891 Titanic passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad134d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"http://patbaa.web.elte.hu/physdm/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030927",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 24), facecolor='black')\n",
    "\n",
    "# Determine the image extent and axis limits for dear Mr. Matplotlib\n",
    "x_lim = (0, X.values.shape[0]-1)\n",
    "y_lim = [-0.5, X.values.shape[1]-0.5]\n",
    "\n",
    "ax.imshow(X.isna().values.T,\n",
    "          extent=(x_lim[0], x_lim[-1], y_lim[0], y_lim[-1]),\n",
    "          aspect=10, cmap=\"Greys\", interpolation='none')\n",
    "\n",
    "# Y-AXIS FORMATTING\n",
    "ax.set_yticks(range(X.columns.size))\n",
    "ax.set_yticklabels(X.columns[::-1], ha='right')\n",
    "ax.tick_params(axis='both', which='major',\n",
    "               labelsize=12, pad=10, colors='white')\n",
    "\n",
    "ax.grid(True, axis='y', ls='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234393b",
   "metadata": {},
   "source": [
    "**This dataset needs some preprocessing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f0136",
   "metadata": {},
   "source": [
    "### A completely different type of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = \"MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEK\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19003b",
   "metadata": {},
   "source": [
    "<img src=\"./images/alphafold.png\"/></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697363e",
   "metadata": {},
   "source": [
    "# How to approach and handle a problem in data science?\n",
    "\n",
    "Most of the problems should be approached and treated similarly by following these simple steps:\n",
    "- Step 1.: Preprocess the dataset for analysis\n",
    "- Step 2.: Find, tune and fit a model or models on the preprocessed dataset\n",
    "- Step 3.: Make predictions using the trained model and evaluate and interpret the results\n",
    "\n",
    "<img src=\"./images/pipeline-full.png\"/></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672118c6",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "A lot of beginner machine learning/data science guide for specific datasets will tell you to work with the data in a very specific way without actually telling you **why** should you do it **that** way? Why *scaling* the data is necessary? Why should you use *hot encoding*? What else can be done about missing data entries besides simply dropping them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e5f7",
   "metadata": {},
   "source": [
    "## 1.0. Every data preprocessing starts with data exploration\n",
    "\n",
    "### Why? Because looking at the data could be extremely insightful...\n",
    "\n",
    "See this example at https://en.wikipedia.org/wiki/Anscombe's_quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sns.load_dataset(\"anscombe\")  # Load Anscombe's quartet from seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976210ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 4\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(5*nc, 5*nr), dpi=120)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for di, ax in zip(X.dataset.unique(), axes.flat):\n",
    "    Xs = X.query(f\"dataset == '{di}'\")['x']\n",
    "    ys = X.query(f\"dataset == '{di}'\")['y']\n",
    "    ax.scatter(Xs, ys, s=15**2)\n",
    "    xi = np.linspace(-10, 25)\n",
    "    yi = 1/2 * xi + 3.0\n",
    "    ax.plot(xi, yi, color='tab:red', lw=5, alpha=0.7)\n",
    "    ax.set_xlim(0.8 * np.min(Xs), 1.1 * np.max(Xs))\n",
    "    ax.set_ylim(0.8 * np.min(ys), 1.1 * np.max(ys))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860af3",
   "metadata": {},
   "source": [
    "#### 1.0.1. Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966970ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"http://patbaa.web.elte.hu/physdm/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827a5a2",
   "metadata": {},
   "source": [
    "### 1.0.2. Exploring missing data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e681cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 24), facecolor='black')\n",
    "\n",
    "# Determine the image extent and axis limits for dear Mr. Matplotlib\n",
    "x_lim = (0, X.values.shape[0]-1)\n",
    "y_lim = [-0.5, X.values.shape[1]-0.5]\n",
    "\n",
    "ax.imshow(X.isna().values.T,\n",
    "          extent=(x_lim[0], x_lim[-1], y_lim[0], y_lim[-1]),\n",
    "          aspect=10, cmap=\"Greys\", interpolation='none')\n",
    "\n",
    "# Y-AXIS FORMATTING\n",
    "ax.set_yticks(range(X.columns.size))\n",
    "ax.set_yticklabels(X.columns[::-1], ha='right')\n",
    "ax.tick_params(axis='both', which='major',\n",
    "               labelsize=15, pad=10, colors='white')\n",
    "\n",
    "ax.grid(True, axis='y', ls='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a70906",
   "metadata": {},
   "source": [
    "### 1.0.3. Exploring datatypes in the dataset\n",
    "\n",
    "Object? Int? Float? Other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `dtypes` variable of a pandas DataFrame object stores the datatypes\n",
    "# of the columns in a specific DataFrame object\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d383c",
   "metadata": {},
   "source": [
    "### 1.0.4. Exploring distribution of feature values\n",
    "\n",
    "Explore a randomly generated classification dataset with 2 distinct classes and 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=8,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc, nr = 4, 2\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(6*nc, 5*nr))\n",
    "\n",
    "mask = np.bool_(y)\n",
    "data = [X[mask], X[~mask]]\n",
    "cmap = [cm.Reds, cm.Blues]\n",
    "labl = ['Class 0', 'Class 1']\n",
    "for d, c, l in zip(data, cmap, labl):\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "        # Convention for plotting numpy.histogram results\n",
    "        hist, bins = np.histogram(d.values[:, i], bins=20, density=True)\n",
    "        width = 0.8 * (bins[1] - bins[0])\n",
    "        center = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.bar(center, hist, width=width, label=l,\n",
    "               color=c(0.6), alpha=0.6)\n",
    "\n",
    "        ax.set_title(f\"Feature {i}\", fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='upper left', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657e051",
   "metadata": {},
   "source": [
    "### 1.0.5. Exploring the correlation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=100,    # Number of points in the data set\n",
    "    n_features=6,     # Number of features in the data set\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8395985",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2333a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    X,\n",
    "    kind='scatter',\n",
    "    diag_kind='kde'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b1bb1",
   "metadata": {},
   "source": [
    "## 1.1. Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d0e5c",
   "metadata": {},
   "source": [
    "### 1.1.1. Deleting rows or columns with too much NaN values\n",
    "\n",
    "- Rows or columns with too many missing values cannot be filled up in a meaningful way that is representative of the underlying distribution of the data\n",
    "- It is better to simply drop rows or columns with too many missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a087bb5",
   "metadata": {},
   "source": [
    "### 1.1.2. Filling empty entries with values\n",
    "\n",
    "- Filling NaN entries with mean of existing values\n",
    "- Filling NaN entries with values sampled from the distribution of existing ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4030e9",
   "metadata": {},
   "source": [
    "## 1.2. Handling non-numeric data\n",
    "\n",
    "- E.g. utilize label encoding and one-hot encoding\n",
    "- Or do nothing with them in case of using various tree ensemble methods (e.g. TabNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186260eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "categories = ['Dog', 'Cat', 'Raccoon', 'Bear']\n",
    "feat = np.random.choice(categories, size=100, replace=True)\n",
    "X[5] = pd.Series(feat)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c4851",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "\n",
    "Used, when the categorical feature is ordinal, i.e. the values have a natural order.\n",
    "- E.g. \"rite\", \"cum laude\", \"summa cum laude\" can be encoded to 0, 1, 2\n",
    "- E.g. \"small\", \"medium\", \"large\", \"extra large\" can be encoded to 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Xenc = le.fit_transform(X.iloc[:, 5])\n",
    "pd.concat([X.iloc[:, :-1], pd.Series(Xenc, name=X.columns[5])], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d5943",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Used, when the categorical feature is nominal, i.e. the values do not have a natural order.\n",
    "- E.g. \"red\", \"green\", \"blue\" can be encoded to 3 binary features\n",
    "- E.g. \"bear\", \"cat\", \"dog\", \"raccoon\" can be encoded to 4 binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1.\n",
    "encoder = OneHotEncoder(categories=[categories], sparse_output=False)\n",
    "Xenc = encoder.fit_transform(X.iloc[:, 5].values.reshape(-1, 1))\n",
    "Xenc = pd.DataFrame(Xenc.astype(int), columns=categories)\n",
    "pd.concat((X.iloc[:, :-1], Xenc), axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de227c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2.\n",
    "Xenc = pd.get_dummies(X.iloc[:, -1], dtype=int)\n",
    "pd.concat([X.iloc[:, :-1], Xenc], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b2a40",
   "metadata": {},
   "source": [
    "### Why can (and should) we use these two methods in these two different cases?\n",
    "\n",
    "In the case of **one-hot encoding**, we prevent the model from learning false correlations between different values of a categorical feature. If we instead assigned ascending integer values to these categories, we would be mapping them to a **discrete, ordered scale**. Statistical models would then interpret this as if a meaningful order exists&mdash;for example, that if `0` is assigned to \"small\" and `1` to \"medium\", then \"small\" $<$ \"medium\", which is **correct** when the categories are **ordinal**.\n",
    "\n",
    "However, this logic falls apart when applied to **nominal** categories such as \"red\", \"green\", and \"blue\". Assigning `1` to \"green\" and `2` to \"blue\" would incorrectly suggest to the model that \"green\" $<$ \"blue\"&mdash;which is not only **nonsensical**, but potentially **harmful** to model performance (unless you are a color zealot, in which case that is a separate issue and we will NOT ask your opinion about this question).\n",
    "\n",
    "That is why **one-hot encoding** is used for nominal features: it treats each category as **independent**, avoiding any unintended assumptions about order or distance between values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f5dc7",
   "metadata": {},
   "source": [
    "## 1.3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a1d7a",
   "metadata": {},
   "source": [
    "### 1.3.1 Example using an SDSS plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f971d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = SDSS.get_images(coordinates=co)\n",
    "X = pd.DataFrame(imgs[0][0].data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6756d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(X.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1acdd6",
   "metadata": {},
   "source": [
    "### Machine learning algorithms don't like values all over the scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44035556",
   "metadata": {},
   "source": [
    "#### StandardScaler\n",
    "\n",
    "Scales each individual **feature (column)** to have a mean of $0$ and a standard deviation of $1$.\n",
    "\n",
    "$$\n",
    "    x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of each feature. This is useful when features have different scales or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = StandardScaler().fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cad3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(Xs.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ea67d",
   "metadata": {},
   "source": [
    "#### Normalizer  \n",
    "Scales each individual **sample (row)** to have unit norm, meaning the entire row vector has length $1$. This is often useful when the direction of the data matters more than its magnitude. Normalizer uses either L1 or L2 norm to scale the data.\n",
    "\n",
    "- **L2 norm** (Euclidean norm): scales the vector so that\n",
    "  $$\n",
    "      \\| \\mathbf{x} \\|_{2} = \\sqrt{x_{1}^{2} + x_{2}^{2} + \\cdots + x_{n}^{2}} = 1\\,.\n",
    "  $$\n",
    "  Each element is then divided by the L2 norm of the vector:\n",
    "  $$\n",
    "      x'_i = \\frac{x_{i}}{\\| \\mathbf{x} \\|_2}\n",
    "  $$\n",
    "\n",
    "- **L1 norm** (Manhattan norm): scales the vector so that\n",
    "  $$\n",
    "      \\| \\mathbf{x} \\|_{1} = |x_{1}| + |x_{2}| + \\cdots + |x_{n}| = 1\\,.\n",
    "  $$\n",
    "  Each element is then divided by the L1 norm of the vector:\n",
    "  $$\n",
    "      x'_i = \\frac{x_{i}}{\\| \\mathbf{x} \\|_{1}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95affb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn = Normalizer(norm='l2').fit_transform(X)\n",
    "Xn = pd.DataFrame(Xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b53935",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(Xn.values.flatten(), color='indianred', lw=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.get_offset_text().set_fontsize(30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03938d05",
   "metadata": {},
   "source": [
    "#### Images themselves need a different type of normalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf24050",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*10, nr*10))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.025,\n",
    "                    left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "ax = axes[0]\n",
    "ax.axis(False)\n",
    "ax.imshow(Xs.values, origin='lower', cmap='gray')\n",
    "ax.text(0.025, 0.975, 'Scaled Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.axis(False)\n",
    "ax.imshow(Xn.values, origin='lower', cmap='gray')\n",
    "ax.text(0.025, 0.975, 'Normalized Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axis(False)\n",
    "norm = ImageNormalize(X.values, interval=ZScaleInterval())\n",
    "ax.imshow(X.values, origin='lower', cmap='gray', norm=norm)\n",
    "ax.text(0.025, 0.975, 'IRAF\\'s zscale', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d748",
   "metadata": {},
   "source": [
    "### 1.3.2 Example using a 2D map from the CAMELS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20352c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a larger image file (~100 Mb), it could take a while to download,\n",
    "# depending on your internet connection and the server load!\n",
    "#\n",
    "# URL might change in the future, search for the latest version\n",
    "# at https://camels-multifield-dataset.readthedocs.io/en/latest/access.html\n",
    "url = 'https://users.flatironinstitute.org/~fvillaescusa/priv/DEPnzxoWlaTQ6CjrXqsm0vYi8L7Jy/CMD/2D_maps/data/IllustrisTNG/Maps_Mtot_IllustrisTNG_CV_z=0.00.npy'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Immediately throws error if the download failed\n",
    "\n",
    "X = np.load(io.BytesIO(response.content))\n",
    "X = pd.DataFrame(X.reshape((-1, 256**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*6, nr*6), dpi=120)\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.025,\n",
    "                    left=0.05, right=0.95, bottom=0.05, top=0.95)\n",
    "\n",
    "img = X.iloc[X.sum(axis=1).argmax()].values.reshape((256, 256))\n",
    "cmap = 'magma'\n",
    "\n",
    "ax = axes[0]\n",
    "ax.axis(False)\n",
    "ax.imshow(img, cmap=cmap)\n",
    "ax.text(0.025, 0.975, 'Raw Image', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.axis(False)\n",
    "ax.imshow(np.log10(img), cmap=cmap)\n",
    "ax.text(0.025, 0.975, '$\\\\log_{10}$', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axis(False)\n",
    "norm = ImageNormalize(img, interval=ZScaleInterval())\n",
    "ax.imshow(img, cmap=cmap, norm=norm)\n",
    "ax.text(0.025, 0.975, 'IRAF\\'s zscale', color='white', fontweight='bold',\n",
    "        fontsize=30, ha='left', va='top', transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798e652-7d34-4e01-813d-3fda9bc566e4",
   "metadata": {},
   "source": [
    "## 1.4. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5caab-41ec-47bb-8209-d75d755e0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62813bda-46ec-4e03-be46-ac1feb339373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...in the next section..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6d716e-43e5-4a84-bcd2-ca017dcdc08e",
   "metadata": {},
   "source": [
    "# Data science in Python III. - Neural Networks (NNs) and Deep Learning (DL)\n",
    "\n",
    "<center>\n",
    "  <img width=\"545\" height=\"300\" src=\"./images/ann.png\"/>\n",
    "  <img width=\"500\" height=\"300\" src=\"./images/alphafold-net.png\"/>\n",
    "</center>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 1. A simple Deep Neural Network (DNN) vs the structure of AlphaFold2</b>\n",
    "</p>\n",
    "\n",
    "### Glossary\n",
    "<p>\n",
    "  Just like other terms related to different fields in machine learning and artificial intelligence, the terms related to neural networks are also not well defined.\n",
    "</p>\n",
    "<ul>\n",
    "<li><b>Neural Network (NN)</b> (<i>hu: neurális háló(zat)</i> ): An umbrella term for NNs and NN-based algorithms in machine learning.</li>\n",
    "\n",
    "<li><b>Artificial Neural Network (ANN)</b> (<i>hu: mesterséges neurális háló(zat)</i> ): The \"correct\" term describing the most basic computational model of biological neural networks. An ANN consist of a set of connected nodes called <i>artificial neurons</i>, that can input multiple signals, process them, then output a single signal, where all of them are represented by real numbers in computing (although complex-valued NNs do exists, but they never really progressed beyond the conceptual/experimental level). The output value of a single neuron is calculated by sending the weighted sum of all input signals through a non-linear function. Typically, neurons are aggregated into layers, where the vector if the input data is simply called as <i>input layer</i> and the output value (in case of regression) or values (in case of classification) is called as the <i>output layer</i>. The layers of neurons between these two are referred to as <b>hidden (neuron) layers</b>.</li>\n",
    "\n",
    "<li><b>Deep Neural Network (DNN)</b> (<i>hu: mély neurális háló(zat)</i> ): Artificial Neural Networks with more, than one hidden neuron layers are usually referred to as DNNs, however this differentiation is purely just informative. Neural networks with more, than one neuron layers are widely referred to simply as ANNs too in the scientific literature.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c0567-d5f5-4f17-b866-8160f767286a",
   "metadata": {},
   "source": [
    "## Very short history and definitions\n",
    "Understanding <i>how</i> neural networks work is simple and only requires the understanding of some basic concepts in linear algebra (operations on vectors, matrices and tensors) and calculus (differentiation). However understanding <i>why</i> neural networks work requires a much broader knowledge in mathematics, especially in logics, real analysis and category theory.\n",
    "\n",
    "Warren McCulloch and Walter Pitts (the prior is a neurophysiologist and a latter is an autodidact mathematician working in computational neurophysiology) were those, who devised the idea to describe brain activity on terms of propositional calculus. Their paper from 1943, <i>A logical calculus of the ideas immanent in nervous activity</i> is considered to be the very first article that details the basics of the mathematical description of biological (and also artificial) neural networks or more like the neurons itself. (That's why artificial neurons are referred to as <i>McCulloch-Pitts neurons</i> too sometimes.)\n",
    "\n",
    "By principle, a biological neuron can be modelled as a computational unit, that processes input signals by calculating their weighted sum first (referred to as the <b>linear</b> part), then passing this sum through a so called <b>activation</b> or <b> activation function</b> (referred to as the <b>non-linear</b> part). Optionally a constant value (called as <b>bias</b>, denoted by $b$) can be also added to the summation's result. Artificial neurons try to mimic this exact behaviour. In mathematical terms, the behaviour of a neuron for $N$ number of input signals can be described as\n",
    "\n",
    "$$\n",
    "y\n",
    "=\n",
    "f \\left(\n",
    "  \\sum_{i=1}^{N} w_{i} x_{i} + b\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where $x_{i}$ are the input values, $w_{i}$ are their corresponding weights given by the neuron to them during the weighted summation, $b$ is the arbitrary bias and $f()$ is the non-linear activation function.\n",
    "\n",
    "<center>\n",
    "  <img width=70% src=\"./images/artificial-neuron.png\"/>\n",
    "</center>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 2. A biological neuron and its mathematical model, an artificial neuron</b>\n",
    "</p>\n",
    "\n",
    "The first real artificial neuron was constructed between 1957 and 1958 by <a href=\"https://en.wikipedia.org/wiki/Frank_Rosenblatt\">Frank Rosenblatt</a> and his research team at the Cornell Aeronautical Laboratory [[1]](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf). It was an actual, wardrobe-sized machine, called the <i>Mark I Perceptron</i> that implemented the <i>perceptron</i> algorithm, a rudimentary binary classifier containing a single artificial neuron. In this setup, all the inputs are directly fed into the output layer through a single weighted summation and activation function (so a neuron). The perceptron algorithm's \"binary\" nature arises from the fact that it uses a Heaviside step function as it's activation function, mapping all input vectors to the $\\{0,\\,1\\}$ set, which \n",
    "means it will classify any input to two separate classes (denoted by $0$ and $1$).\n",
    "\n",
    "Adding more neurons to the system, where the input goes into all of the available neurons and from them to the output layer, creates a single-layer neural network. Similarly, adding more layers of neurons, where the inputs of each new layers are the outputs of the previous ones, makes a multi-layer neural network. These type of networks - just like the Mark I Perceptron - are referred to as <b>feedforward</b> neural networks that means \"signals propagate through the network in a single direction, without any loops\". However after the success of the Perceptron machine, a huge decline in the interest for neural networks was observed. The main reason for this was the non-versatile nature of feedforward networks.\n",
    "\n",
    "After a short period of slow research in the 1960s, the final nail in the coffin of neural networks came in 1969, when the book titled <i>Perceptrons</i> by Marvin Minsky and Seymour Papert showed that it was impossible for single-layer feedforward networks to learn an XOR function, which is also referred to as the <i>XOR problem</i> and can be seen on Fig. 3. Unfortunately human laziness already existed that time and without carefully reading their book, people often believed that Minsky and Papert proved this non-versatility for multi-layer neural networks too. Although this wasn't true and they've explicitly conjectured that the XOR problem can be solved using multi-layer networks, it completely halt neural network research over the next $\\approx 25$ years. (Yes, this really happened.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdb30d-04b5-4975-ad78-1c0840c01015",
   "metadata": {},
   "source": [
    "## Modelling an Artificial Neural Network\n",
    "The parallel linear and non-linear steps performed by each neuron in consecutive neuron layers, as well as the entirety of the backward propagation can be described using basic tools in linear algebra. If we consider $N$ (upper case) number of input signals that we denote with the $N$-dimensional vector $\\boldsymbol{x}$ and a neuron layer with $n$ (lower case) number of neurons, we'll have $N \\times n$ number of weights ($1$ weight for every input in every neuron) and $n$ number of biases ($1$ bias in every neuron). In this case weights can be denoted by the $N \\times n$ matrix $W$ and biases can be denoted by the $n$-dimensional vector $\\boldsymbol{b}$.\n",
    "<center>\n",
    "  <img width=95% src=\"./images/nn_forward.png\"/>\n",
    "</center>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 4. The forward propagation section of an ANN with $L$ number of layer</b>\n",
    "</p>\n",
    "<center>\n",
    "  <img width=95% src=\"./images/nn_backward.png\"/>\n",
    "</center>\n",
    "<p style=\"text-align:center; font-size:24px;\">\n",
    "  <b>Fig. 5. The backward propagation section of an ANN with $L$ number of layer</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e58c49-e37c-437f-8fd4-dc0fa5a0f9a7",
   "metadata": {},
   "source": [
    "## The 2nd main part - Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2eb2b3-b9d1-450e-80b3-f6704a656ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification, \\\n",
    "                             make_blobs, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d25447-a88d-46bc-94ab-9606d52000d9",
   "metadata": {},
   "source": [
    "## Using NumPy\n",
    "\n",
    "The NumPy Python library offers a wide range of tools for numerical calculations. Because NumPy implements full vectorization and a versatile vector class (`numpy.ndarray`), which are not available in standard Python, it has become probably the most popular and most widely used Python library in Python 2.x and above. These core features that I've mentioned, makes NumPy the best choice to perform calculations that primarily involve linear algebra. Since ANNs contain purely linear algebraic calculations, NumPy is the perfect library to implement ANNs from scratch with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3953c-7841-4d2a-bfbf-7de1bca59e76",
   "metadata": {},
   "source": [
    "### Constructing a basic ANN from scratch\n",
    "\n",
    "```python\n",
    "def init_weights():\n",
    "    '''Initializes neuron weights and biases at the start of the training.'''\n",
    "    ...\n",
    "\n",
    "def nn_forward():\n",
    "    '''Implements the forward propagation steps.'''\n",
    "    ...\n",
    "\n",
    "def nn_backward():\n",
    "    '''Implements the backward propagation steps.'''\n",
    "    ...\n",
    "```\n",
    "\n",
    "The steps above require the help of other functions to work, like calculating the activation and the cost/lost function and updating weights and biases during backward propagation:\n",
    "```python\n",
    "def activation():\n",
    "    ...\n",
    "  \n",
    "def loss_function():\n",
    "    ...\n",
    "\n",
    "def update_weights():\n",
    "    '''\n",
    "    Updates the weights and biases of the neurons in the neural network.\n",
    "    Also referred to as \"optimization\".\n",
    "\n",
    "    '''\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a11d68-7dcb-4110-b718-99795aa888b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(ld):\n",
    "    '''\n",
    "    Initializes neuron weights and biases at the start of the training.\n",
    "    Weights are set to be a random, small number, while biases are all\n",
    "    set to 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ld : numpy.ndarray or array-like of shape (N+1,)\n",
    "        Defines the dimensionality of the input data and the number of\n",
    "        neurons in each of the ANN layers. Starts at the input side and\n",
    "        ends at the output side.\n",
    "        \n",
    "        The first element is the dimensionality of the input data, while\n",
    "        its other elements define the number of neutrons in each layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : dict\n",
    "        The initial weights and biases of each neuron layer.\n",
    "    '''\n",
    "    # Initial checks whether input is OK or not\n",
    "    if not isinstance(ld, np.ndarray):\n",
    "        ld = np.array(ld)\n",
    "    assert ld.ndim == 1, \"Input should be 1D array of shape (N,)!\"\n",
    "    assert ld.size > 1, \"The ANN should contain at least a single layer!\"\n",
    "    assert ld[ld == 0].size == 0, \"All layers should cointain at least 1 neuron!\"\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    for li in range(1, len(ld)):\n",
    "        params[f'W{li}'] = np.random.randn(ld[li], ld[li-1]) * 0.01\n",
    "        params[f'b{li}'] = np.zeros((ld[li], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c93ff8-0e58-4f67-986e-9e8f49d0fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "params = init_weights([3, 4, 8, 16, 1])\n",
    "print(f\"keys : {params.keys()}\\n\")\n",
    "print(f\"W1 =\\n{params['W1']}\")\n",
    "print(f\"b1 =\\n{params['b1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e532f-a6f2-40df-8614-c4fa89b8c7fc",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78ba36-080a-45af-b209-93389c1fd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(Z, *, func=None, deriv=False):\n",
    "    '''\n",
    "    Implements the activation node for both the forward and backward\n",
    "    propagation section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : float\n",
    "        Outputs of a neuron layer.\n",
    "    func : str\n",
    "        The choosen activation function. Possible values are 'linear', \n",
    "        'relu', 'sigmoid', 'tanh' or 'softmax'.\n",
    "    deriv : bool\n",
    "        Changes between the activation function and its derivative.\n",
    "        During forward propagation the regular functions, while during\n",
    "        the backward propagation, the derivative should be used.\n",
    "    '''\n",
    "    __all__ = ['linear', 'relu', 'tanh', 'sigmoid', 'softmax']\n",
    "    assert (func in __all__) | (func is None), \\\n",
    "        \"Choosen activation is not implemented!\\n\" \\\n",
    "        + f\"Available options are: {__all__}\"\n",
    "    \n",
    "    if func == 'linear':\n",
    "        if deriv:\n",
    "            return np.ones_like(Z)\n",
    "        else:\n",
    "            return Z\n",
    "    elif func == 'relu':\n",
    "        if deriv:\n",
    "            return 1 * (Z > 0)\n",
    "        return Z * (Z > 0)\n",
    "    elif func == 'sigmoid':\n",
    "        s = lambda x: 1 / (1 + np.exp(-x))\n",
    "        if deriv:\n",
    "            return s(Z) * (1 - s(Z))\n",
    "        return s(Z)\n",
    "    elif func == 'tanh':\n",
    "        if deriv:\n",
    "            return 1 - Z**2\n",
    "        return np.tanh(Z)\n",
    "    elif func == 'softmax':\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb25b0-7513-4dab-b012-54833651cd33",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "The two functions below implements the forward propagation section in an ANN. The `nn_forward_step` calculates the output of a single neuron layer, while the `nn_forward` function links the whole section together. The steps implemented by these functions are the following:\n",
    "\n",
    "1. `nn_forward` is the main function that is called at the beginning of the forward propagation during the training of the neural network. Its argument `X` is the input training data, the `parameters` contains the weigths and biases of the neurons in the network, layer by layer, while the `actv` argument defines, which activation function is used in the hidden layers. The possible activation functions are defined in the function named `activation` above.\n",
    "\n",
    "2. As it was already mentioned at the beginning of this NumPy section, this Python library possesses a versatile tool: the `numpy.ndarray` class. This could help us a lot in our calculations, thus we're first converting any input data to a `numpy.ndarray` type. At the beginning of the `nn_foward` function, the following test is evaluated:\n",
    "\n",
    "    ```python\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "    ```\n",
    "    \\\n",
    "    If our input data, this array-like structure is already a `numpy.ndarray` object, then nothing happens. However if it isn't, then it's getting converted to `numpy.ndarray` first.\n",
    "\n",
    "3. A list named `cache` is created at the line\n",
    "\n",
    "    ```python\n",
    "        cache = []\n",
    "    ```\n",
    "    \\\n",
    "    This `cache` list will contain all the necessary values for the backward propagation section, where derivatives are calculated using the output `a` and `Z` values, that are calculated during the forward propagation, also the existing `W` weights and `b` biases are getting updated. While we have the `W` and `b` values in the `parameters` dictionary, and `a` values can be calculated from the corresponding `Z` values if the `Z` values are saved into `cache` during forward propagation. Just to make everything easier (and our code a bit more transparent), it's best to just save every necessary value to this `cache` list and go on with our lives.\n",
    "\n",
    "4. The number of layers are calculated at the line\n",
    "\n",
    "    ```python\n",
    "        L = len(parameters) // 2\n",
    "    ```\n",
    "    \\\n",
    "    Since the `parameters` dictionary contains exactly one `W` and a `b` entry for every layer, then the length of `parameters` is two-times the number of layers in the ANN.\n",
    "\n",
    "5. The actual forward propagation section begins here. As the computation graph shows on Fig. 2., the input data is transposed before it's passed to the very first neuron layer. After this, the for loop propagates the values through the network, layer by layer, saves the output $Z^{\\left[ i \\right]}$, the weights $W^{\\left[ i \\right]}$ and the biases $b^{\\left[ i \\right]}$ to the cache, then applies the selected activation function (specified by the `actv` parameter) on the $Z$ output.\n",
    "\n",
    "    Calculation of the linear part that is\n",
    "\n",
    "    $$\n",
    "    Z^{\\left[ i \\right]}\n",
    "    =\n",
    "    W^{\\left[ i \\right]} \\cdot a^{\\left[ i - 1 \\right]} + b^{\\left[ i \\right]}\n",
    "    $$\n",
    "    is performed by the `nn_forward_step` function, which waits for the input vector `a`, the weights `W` and biases `b` as its arguments. The reason why the input vector is denoted by `a` can be understood by looking at the computation graph above. Except for the very first layer, where the input is the transpose of the training data, every layer uses the output of the activation function from the previous layer (as it's stated in the equation above also)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aacdbe-4c36-4fce-a0af-1b50f921d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_step(a, W, b):\n",
    "    '''\n",
    "    Implements the linear forward propagation step in an ANN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array-like of shape (1, N)\n",
    "        The output of the previous activation \n",
    "    W : array-like of shape (N, M)\n",
    "    \n",
    "    b : array-like of shape (M, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Z : numpy.ndarray\n",
    "        \n",
    "    '''\n",
    "    Z = np.matmul(W, a) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68164d57-5b7f-4f9f-a761-de513eddeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "X = np.random.random((10, 30))\n",
    "params = init_weights([30, 4])\n",
    "Z = nn_forward_step(X.T, params['W1'], params['b1'])\n",
    "a = activation(Z.T, func='softmax')\n",
    "print(f\"{Z = }\")\n",
    "print(f\"{a = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b69bb-adc7-417e-8fec-0b37a5241706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward(X, *, params, actv: str = 'relu'):\n",
    "    '''\n",
    "    Implements consecutive forward propagation steps in an ANN.\n",
    "    Input values propagated through the neural network layer by layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray or array-like of shape (M, N)\n",
    "        An input entry in an ANN is in the form of a vector. Mutiple\n",
    "        input dataentries can be sorted into a table of size M by N,\n",
    "        where M is the number of rows that represent the individual\n",
    "        input entries, while N is the number of columns that represents\n",
    "        the dimensionality of the input data.\n",
    "    params : dict\n",
    "    \n",
    "    '''\n",
    "    # Initial checks whether input is OK or not\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "        \n",
    "    # Store forward step results (Z), weights (W) and biases (b) for\n",
    "    # the backward propagation steps\n",
    "    cache = []\n",
    "    \n",
    "    # Number of layers is defined in the size of the `parameters` dict\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    # I. Layers 0 -> L-1\n",
    "    ## In the very first layer the input is transposed\n",
    "    a = X.T\n",
    "    for li in range(1, L):\n",
    "        ## Take a single forward step\n",
    "        Wi = params[f'W{li}']\n",
    "        bi = params[f'b{li}']\n",
    "        Z = nn_forward_step(a, Wi, bi)\n",
    "\n",
    "        ## Add relevant values to the `cache` list\n",
    "        cache.append((a, Z, Wi, bi))\n",
    "\n",
    "        ## Apply the activation function\n",
    "        a = activation(Z, func=actv)\n",
    "    \n",
    "    # II. Last layer\n",
    "    ## Take the last forward step\n",
    "    WL = params[f'W{L}']\n",
    "    bL = params[f'b{L}']\n",
    "    Z = nn_forward_step(a, WL, bL)\n",
    "    \n",
    "    ## Add relevant values to the `cache` list\n",
    "    cache.append((a, Z, WL, bL))\n",
    "    \n",
    "    ## At the end of the last layer, the output is transposed before\n",
    "    ## going through the last activation function\n",
    "    Z = Z.T\n",
    "\n",
    "    ## Apply the last activation function\n",
    "    if bL.size == 1:\n",
    "        ## [REGRESSION] Single neuron in the last layer\n",
    "        ##\n",
    "        ## In case of regression, the last neuron layer contains only a\n",
    "        ## single neuron.\n",
    "        a = activation(Z, func='linear')\n",
    "    else:\n",
    "        ## [CLASSIFICATION] Multiple neurons in the last layer\n",
    "        ##\n",
    "        ## In case of classification, the last neuron layer represents\n",
    "        ## the number of classes we want to predict.\n",
    "        a = activation(Z, func='softmax')\n",
    "    \n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8aacc-3cb2-4148-9b99-9aa9fc2d2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "X = np.random.randn(5, 30)\n",
    "params = init_weights([30, 4, 1])\n",
    "a, cache = nn_forward(X, params=params, actv='sigmoid')\n",
    "print(f\"{a = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e115ea-8b03-4eb5-b798-25d835e003ab",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c2249-bdb9-417c-92f5-38511ba71563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_crossentropy(P, y, *, deriv=False):\n",
    "    '''\n",
    "    Calculates the binary cross-entropy.\n",
    "    '''\n",
    "    M = y.shape[0]\n",
    "    \n",
    "    if deriv:\n",
    "        return 1 / M * (P - y).T\n",
    "    return - 1 / M * np.sum(np.multiply(y, np.log(P)))\n",
    "\n",
    "\n",
    "def root_mean_square(a, y, *, deriv=False):\n",
    "    '''\n",
    "    Calculates the root mean square error.\n",
    "    '''\n",
    "    if deriv:\n",
    "        pass\n",
    "    return np.sum(np.sqrt((a - np.mean(y))**2)) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbb650-d248-41fe-a065-28475934d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (regression)\n",
    "y = np.random.random((1,5))\n",
    "loss = root_mean_square(a=a, y=y)\n",
    "print(f\"{y = }\")\n",
    "print(f\"{loss = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fb2a0-a710-4b50-9a06-51bfbf368aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (classification)\n",
    "y = np.eye(2)[np.random.randint(0, 2, 5)]\n",
    "loss = binary_crossentropy(P=a, y=y)\n",
    "print(f\"{y = }\")\n",
    "print(f\"{loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df8c8f-b131-4efb-9039-39cd14ad9791",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "<center>\n",
    "  <img width=95% src=\"./images/nn_backward.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f88fd5-a22c-4b74-a5e0-9861cf3ad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backward_step(dLdZ, cache):\n",
    "    '''\n",
    "    Implements a single backward step in the ANN.\n",
    "    '''\n",
    "    # cache = (a, Z, W, b)\n",
    "    dLda = np.dot(cache[2].T, dLdZ)             # dLda = W.T * dLdZ\n",
    "    dLdW = np.dot(dLdZ, cache[0].T)             # dLdW = dLdZ * a.T\n",
    "    dLdb = np.sum(dLdZ, axis=1, keepdims=True)  # dLdb = sum(dLdZ)\n",
    "    \n",
    "    return dLda, dLdW, dLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49083eff-f444-46f2-a35e-82c375cfb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "np.random.seed(1)\n",
    "params = init_weights([3, 2, 2])\n",
    "X = np.random.randn(3, 3)\n",
    "y = np.eye(2)[np.random.randint(0, 2, 3)]\n",
    "a, cache = nn_forward(X, params=params, actv='sigmoid')\n",
    "\n",
    "dLdZ = binary_crossentropy(a, y, deriv=True)\n",
    "\n",
    "dLda, dLdW, dLdb = nn_backward_step(dLdZ, cache[-1])\n",
    "print(\"dLda=\",dLda)\n",
    "print(\"dLdW=\",dLdW)\n",
    "print(\"dLdb=\",dLdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01fb7f-7d70-43cf-9114-fd23027c548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backward(a, y, cache, actv: str = 'relu'):\n",
    "    '''\n",
    "    Implements consecutive backward propagation steps in an ANN.\n",
    "    '''\n",
    "    # Dictionary to store the derivatives of every layer in\n",
    "    derivatives = {}\n",
    "    \n",
    "    # Number of layers is defined as the length if the cache list\n",
    "    L = len(cache)\n",
    "    \n",
    "    # Calculate the backward loss function to get dLdZ\n",
    "    if cache[-1][-1].size == 1:\n",
    "        dLdZ = root_mean_square(a=a, y=y, deriv=True)\n",
    "    else:\n",
    "        dLdZ = binary_crossentropy(P=a, y=y, deriv=True)\n",
    "    \n",
    "    for i in range(L, 1, -1):\n",
    "        # Calculate and save relevant derivatives from the linear\n",
    "        # backward propagation section in hidden layers\n",
    "        dLda, dLdW, dLdb = nn_backward_step(dLdZ, cache[i - 1])\n",
    "        derivatives[f'dLdW{i}'] = dLdW\n",
    "        derivatives[f'dLdb{i}'] = dLdb\n",
    "        \n",
    "        # a' = f'(Z)\n",
    "        a_prime = activation(cache[i - 2][1], func=actv, deriv=True)\n",
    "        # dLdZ = dLda * a'\n",
    "        dLdZ = np.multiply(dLda, a_prime)\n",
    "\n",
    "    # Calculate and save relevant derivatives from the linear\n",
    "    # backward propagation section in the first layer\n",
    "    dLda, dLdW, dLdb = nn_backward_step(dLdZ, cache[0])\n",
    "    derivatives[f'dLdW1'] = dLdW\n",
    "    derivatives[f'dLdb1'] = dLdb\n",
    "\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cbeae-5fb2-446c-a44e-44db6b93e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "params = init_weights([3, 2, 2])\n",
    "X = np.random.randn(4, 3)\n",
    "y = np.eye(2)[np.random.randint(0, 2, 4)]\n",
    "a, cache = nn_forward(X, params=params, actv='sigmoid')\n",
    "derivatives = nn_backward(a, y, cache, actv='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b0aca-3848-4944-b423-20ceca079fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019054e-184a-4fd7-b6f1-d886b4eb47e5",
   "metadata": {},
   "source": [
    "### Connecting and training the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927b609-8403-4a09-96c6-3c448944a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(X, y, *, ld=None, epochs=50, lr=0.01, actv='sigmoid'):\n",
    "    '''\n",
    "    Train\n",
    "    '''\n",
    "    #\n",
    "    losses = []\n",
    "    \n",
    "    # Initialize the weights and biases of the network\n",
    "    params = init_weights(ld)\n",
    "    \n",
    "    # Number of layers is defined in the size of the `parameters` dict\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    # In one iteration of gradient descent\n",
    "    for l in tqdm(range(epochs)):\n",
    "        \n",
    "        # Propagate the data through the network in a forward direction\n",
    "        a, cache = nn_forward(X, params=params, actv=actv)\n",
    "        \n",
    "        # Calculate the loss and save it into the `lpsses` list for\n",
    "        # later use\n",
    "        if ld[-1] == 1:\n",
    "            loss = root_mean_square(a, y)\n",
    "        else:\n",
    "            loss = binary_crossentropy(a, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Calculate the derivates doing a backward step in the network\n",
    "        derivates = nn_backward(a, y, cache, actv=actv)\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            # Update the weights\n",
    "            params[f\"W{i}\"] = params[f\"W{i}\"] - lr * derivates[f\"dLdW{i}\"]\n",
    "            params[f\"b{i}\"] = params[f\"b{i}\"] - lr * derivates[f\"dLdb{i}\"]\n",
    "\n",
    "    return params, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f30d13-f7d6-4e48-b02d-d23d140a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1280,\n",
    "    n_features=3,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "y = np.eye(2)[y]\n",
    "\n",
    "# Randomly select a test set\n",
    "p_test = 0.33\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f543f-a434-4d12-87ce-d244c8a38bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08698b43-171d-43e1-b38a-9f4f6aa5c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while\n",
    "params, losses = nn_train(\n",
    "    X_train, y_train,\n",
    "    ld=[X.shape[1], 4, 8, 16, 32, 2],\n",
    "    epochs=500,\n",
    "    lr=100,\n",
    "    actv='sigmoid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f8277-b1bd-44e1-aad3-5b7e675b1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(losses, lw=3)\n",
    "ax.set_xlabel('Epochs', fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel('Binary cross-entropy loss', fontsize=20, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7985d-32bc-4a8f-9914-f51c4eef5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, _ = nn_forward(X_test, params=params, actv='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4354a-a16e-4ca6-8bf4-02547f297ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1160327-9e1c-489c-80a1-7e8ac536f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "fig, ax = plt.subplots(figsize=(10,  10))\n",
    "ConfusionMatrixDisplay.from_predictions(np.argmax(y_test, axis=1), y_pred,\n",
    "                                        ax=ax)\n",
    "ax.set_xlabel('Predicted label', fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel('True label', fontsize=20, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d6c34-bfdf-4def-872d-5b1f2270e496",
   "metadata": {},
   "source": [
    "## Using TensorFlow and Keras\n",
    "\n",
    "TensorFlow is an open source machine learning and AI library for Python, C++ and Java, originally developed by Google to help those primarily engaged in general deep learning. It provides an almost block-based approach for anyone to construct deep learning models of any complexity easily. While it's focused on deep neural networks, it can still be used a variety of other tasks. Since TensorFlow 2.0, the production branch of TensorFlow has merged with the Keras library. Keras contains several implementations of neural network modules and layers, optimization methods, activation functions, loss metrics and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429359b1-c980-4cf9-a216-2bc1f11ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as kl\n",
    "from tensorflow.keras import models as km\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import callbacks as kc\n",
    "from tensorflow.keras import optimizers as ko\n",
    "from tensorflow.keras import regularizers as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd1723-0bfe-4d8f-bf13-216ffa62d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow version\n",
    "print(f\"TF version : {tf.__version__}\")\n",
    "# Test if GPU is available for TensorFlow\n",
    "# This should show the available GPUs, listed in an array\n",
    "print(f\"GPU : {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83886895-ed6d-4825-b56e-45851b3b2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_tf(ld,\n",
    "           activation='relu'):\n",
    "    '''\n",
    "    Implements an N-layer ANN in TensorFlow-Keras.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ld : numpy.ndarray or array-like of shape (N+1,)\n",
    "        Defines the dimensionality of the input data and the number of\n",
    "        neurons (N) in each of the ANN layers. Starts at the input side\n",
    "        and ends at the output side.\n",
    "        \n",
    "        The first element is the dimensionality of the input data, while\n",
    "        its other elements define the number of neutrons in each layers.\n",
    "    \n",
    "    activation : str\n",
    "        Specifies the activation function for the hidden layers.\n",
    "    '''\n",
    "    # Initial checks whether input is OK or not\n",
    "    if not isinstance(ld, np.ndarray):\n",
    "        ld = np.array(ld)\n",
    "    assert ld.ndim == 1, \"Input should be 1D array of shape (N,)!\"\n",
    "    assert ld.size > 1, \"The ANN should contain at least a single layer!\"\n",
    "    assert ld[ld == 0].size == 0, \"All layers should cointain at least 1 neuron!\"\n",
    "    \n",
    "    # Tensorflow placeholder for inputs\n",
    "    inp = kl.Input(shape=(ld[0],))\n",
    "    x = inp\n",
    "    \n",
    "    # Define hidden layers of the ANN\n",
    "    for ni in ld[1:-1]:\n",
    "        x = kl.Dense(ni, activation=activation)(x)\n",
    "    \n",
    "    # Define last layer of the ANN\n",
    "    name = f\"final_dense_n{ld[-1]}_ngpu{len(gpu.split(','))}\"\n",
    "    if ld[-1] == 1:\n",
    "        x = kl.Dense(ld[-1], activation='linear', name=name)(x)\n",
    "    else:\n",
    "        x = kl.Dense(ld[-1], activation='softmax', name=name)(x)\n",
    "    \n",
    "    model = km.Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1db088-9310-48bc-8df9-553f5a5eea70",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7f9f2-3eba-44fc-9a5a-c0d90a7b494f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "GPU = [f\"GPU:{i}\" for i in gpu.split(',')]\n",
    "\n",
    "if len(gpu.split(',')) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(GPU)\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(GPU[0])\n",
    "\n",
    "with strategy.scope():\n",
    "    model = ann_tf(ld=np.array([10, 4, 8, 16, 1]), activation='relu')\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eba780-a5d3-402f-885f-153f68670aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7b159-6c26-48f4-ae30-c66dd2c09b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=12800,\n",
    "    n_features=10,\n",
    "    n_informative=4,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Randomly select a test set\n",
    "p_test = 0.33\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33,\n",
    "                     random_state=57)\n",
    "# Randomly select a validation set\n",
    "p_valid = 0.25\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X_train, y_train, test_size=p_valid / (1 - p_test),\n",
    "                     random_state=57)\n",
    "\n",
    "#\n",
    "# TENSORFLOW PURGATORY IN EARLY 2022\n",
    "#\n",
    "\n",
    "# Wrap data in Dataset objects\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "\n",
    "# The batch size must now be set on the Dataset objects\n",
    "batch_size = 128\n",
    "train_data = train_data.batch(batch_size)\n",
    "valid_data = valid_data.batch(batch_size)\n",
    "\n",
    "# Disable AUTO sharding policy\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = \\\n",
    "                        tf.data.experimental.AutoShardPolicy.OFF\n",
    "train_data = train_data.with_options(options)\n",
    "valid_data = valid_data.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1153dd6-1d0a-499c-beec-bc6f9a944208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_data, validation_data=valid_data, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c706702-0d9f-4fcf-a451-c8f08d3f1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b10a0c-40bf-46ab-b3e1-ff3268f16280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9e563-8ae6-4c9d-bb73-ff857146cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cd8af-e822-4609-b34d-42822f4dee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.set_xlabel('Groundtruth', fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel('Prediction', fontsize=20, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86658807-b6f7-4677-aff8-79f9b05a40a4",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12043aa3-41a1-4bb5-b7ec-54f177c62077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "GPU = [f\"GPU:{i}\" for i in gpu.split(',')]\n",
    "\n",
    "if len(gpu.split(',')) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(GPU)\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(GPU[0])\n",
    "\n",
    "with strategy.scope():\n",
    "    model = ann_tf(ld=np.array([10, 4, 8, 16, 2]), activation='sigmoid')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb68d7f-e759-4da1-b3bf-44089247e6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8223524-1d46-4a3c-9689-25e96c2a1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16275723-5cbd-421c-9580-49e016704e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=12800,\n",
    "    n_features=10,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Randomly select a test set\n",
    "p_test = 0.33\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33)\n",
    "# Randomly select a validation set\n",
    "p_valid = 0.25\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X_train, y_train, test_size=p_valid / (1 - p_test))\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = np.eye(2)[y_train]\n",
    "y_valid = np.eye(2)[y_valid]\n",
    "\n",
    "#\n",
    "# TENSORFLOW PURGATORY IN EARLY 2022\n",
    "#\n",
    "# Wrap data in Dataset objects\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "\n",
    "# The batch size must now be set on the Dataset objects\n",
    "batch_size = 128\n",
    "train_data = train_data.batch(batch_size)\n",
    "valid_data = valid_data.batch(batch_size)\n",
    "\n",
    "# Disable AUTO sharding policy\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = \\\n",
    "                        tf.data.experimental.AutoShardPolicy.OFF\n",
    "train_data = train_data.with_options(options)\n",
    "valid_data = valid_data.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb5f53-a434-4587-aec5-aa2baa539dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_data,\n",
    "                    validation_data=valid_data, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cf3b1-1e25-4f7f-8267-5f5c0b070048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51674c30-3207-4c1a-be56-e3500c8477eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9e69d-59d3-43a3-9072-e715b6ce6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "fig, ax = plt.subplots(figsize=(10, \n",
    "                                10))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n",
    "ax.set_xlabel('Predicted label', fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel('True label', fontsize=20, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393be8e9-e702-4075-a7fc-6a40b0a581d2",
   "metadata": {},
   "source": [
    "## Something bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4eceb-f3dc-4fc4-ba16-c3e415f589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d538b1-6105-4c61-b315-d8d606be435e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Z = []\n",
    "\n",
    "for f in natsort.os_sorted(os.listdir('/home/masterdesky/data/SDSS/images/')):\n",
    "    X.append(plt.imread(os.path.join('/home/masterdesky/data/SDSS/images/', f))[:,:,:3])\n",
    "    Z.append(float(f.split('z')[-1][:-4]))\n",
    "\n",
    "X = np.array(X)\n",
    "Z = np.array(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3711c6f-3861-48ce-b923-afc358ce3c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X), size=nrows*ncols)\n",
    "images = X[rand_idx]\n",
    "labels = Z[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i], cmap='Greys_r')\n",
    "    ax.set_title(f'z : {labels[i]}', fontweight='bold',\n",
    "                 color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 4. Sample data along with their labels of the SDSS dataset.',\n",
    "             color='white', fontsize=20, y=0.05)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5315e5-0ac8-49e8-99eb-bd8fcd86a52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "valid_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Z,\n",
    "                          test_size=test_size, random_state=57)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                          test_size=valid_size/(1-test_size), random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424d98f-93b2-4d73-9f7f-6835be0d0a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train :', X_train.shape)\n",
    "print('Valid :', X_valid.shape)\n",
    "print('Test :', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a14d9-e3ed-4fff-9cde-b1beccbb85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(imsize, n_target, stride, kernelsize,\n",
    "        n_channels=1, num_of_filters=32,\n",
    "        padding='same', activation='relu', \n",
    "        reg=5e-5, gpu='0,1,2'):\n",
    "    \n",
    "    # Tensorflow placeholder for inputs\n",
    "    inputs = kl.Input(shape=(imsize, imsize, n_channels))\n",
    "\n",
    "    #\n",
    "    # Convolutional block 1.\n",
    "    # 3x3CONVx32 -> ReLU -> 3x3CONVx32 -> ReLU -> MAXPOOLx2\n",
    "    #\n",
    "    x = kl.Conv2D(filters=num_of_filters,                   # 3x3CONVx32\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(inputs)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=num_of_filters,                   # 3x3CONVx32\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.MaxPooling2D(strides=(2, 2))(x)                  # MAXPOOLx2\n",
    "\n",
    "\n",
    "    #\n",
    "    # Convolutional block 2.\n",
    "    # 3x3CONVx64 -> ReLU -> 3x3CONVx64 -> ReLU -> MAXPOOLx2\n",
    "    #\n",
    "    x = kl.Conv2D(filters=2*num_of_filters,                 # 3x3CONVx64\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=2*num_of_filters,                 # 3x3CONVx64\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.MaxPooling2D(strides=(2, 2))(x)                  # MAXPOOLx2\n",
    "\n",
    "\n",
    "    #\n",
    "    # Convolutional block 3.\n",
    "    # 3x3CONVx128 -> ReLU -> 1x1CONVx64 -> ReLU -> 3x3CONVx128 -> ReLU -> MAXPOOLx2\n",
    "    #\n",
    "    x = kl.Conv2D(filters=4*num_of_filters,                 # 3x3CONVx128\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=2*num_of_filters,                 # 1x1CONVx64\n",
    "                kernel_size=(1, 1),\n",
    "                padding=padding,\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))\n",
    "\n",
    "    x = kl.Conv2D(filters=4*num_of_filters,                 # 3x3CONVx128\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.MaxPooling2D(strides=(2, 2))(x)                  # MAXPOOLx2\n",
    "\n",
    "\n",
    "    #\n",
    "    # Convolutional block 4.\n",
    "    # 3x3CONVx256 -> ReLU -> 1x1CONVx128 -> ReLU -> 3x3CONVx256 -> ReLU -> MAXPOOLx2\n",
    "    #\n",
    "    x = kl.Conv2D(filters=8*num_of_filters,                 # 3x3CONVx256\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=4*num_of_filters,                 # 1x1CONVx128\n",
    "                kernel_size=(1, 1),\n",
    "                padding=padding,\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=8*num_of_filters,                 # 3x3CONVx256\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.MaxPooling2D(strides=(2, 2))(x)                  # MAXPOOLx2\n",
    "\n",
    "\n",
    "    #\n",
    "    # Convolutional block 5.\n",
    "    # 3x3CONVx512 -> ReLU -> 1x1CONVx256 -> ReLU -> 3x3CONVx512 -> ReLU -> AVGPOOL ||\n",
    "    #\n",
    "    x = kl.Conv2D(filters=16*num_of_filters,                # 3x3CONVx512\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=8*num_of_filters,                 # 1x1CONVx256\n",
    "                kernel_size=(1, 1),\n",
    "                padding=padding,\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "    x = kl.Conv2D(filters=16*num_of_filters,                # 3x3CONVx512\n",
    "                kernel_size=(kernelsize, kernelsize),\n",
    "                padding=padding,\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=kr.l2(reg))(x)\n",
    "    x = kl.Activation(activation)(kl.BatchNormalization()(x))   # ReLU\n",
    "\n",
    "\n",
    "    # End of convolution\n",
    "    x = kl.GlobalAveragePooling2D()(x)                      # AVGPOOL\n",
    "\n",
    "    x = kl.Dense(units=n_target,\n",
    "                 name = f\"final_dense_n{n_target}_ngpu{len(gpu.split(','))}\")(x)\n",
    "\n",
    "    model = km.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39720307-3606-4ec0-ba45-56d4da67e141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "GPU = [f\"GPU:{i}\" for i in gpu.split(',')]\n",
    "\n",
    "if len(gpu.split(',')) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(GPU)\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(GPU[0])\n",
    "\n",
    "with strategy.scope():\n",
    "    best_model = kc.ModelCheckpoint('./best_model.hdf5',\n",
    "                                    save_best_only=True, verbose=1)\n",
    "    model = CNN(\n",
    "        imsize=X[0].shape[1], n_target=1, stride=1, kernelsize=3,\n",
    "        n_channels=3, num_of_filters=32,\n",
    "        padding='same', activation='relu', \n",
    "        reg=5e-5, gpu=gpu\n",
    "    )\n",
    "    model.compile(optimizer=ko.Adam(learning_rate=5e-5),\n",
    "                  loss='MeanSquaredError',\n",
    "                  metrics=['MeanAbsoluteError'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc2036-c481-4528-8d2f-048b03069439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2c3e9-1f28-41db-b6b4-195e882b188d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "history = model.fit(x=X_train, y=y_train,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                    callbacks=[best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727de0d6-a206-4a89-986e-2ee8b33ed6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "\n",
    "x = np.arange(epochs)+1\n",
    "ax.plot(x, history.history['loss'], label='Train loss',\n",
    "        color='tab:blue', lw=4, alpha=0.9)\n",
    "ax.plot(x, history.history['val_loss'], label='Valid. loss',\n",
    "        color='tab:orange', lw=4, alpha=0.9)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('Epoch', fontsize=15, fontweight='bold')\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=14, ncol=2)\n",
    "ax.grid(ls='--', color='0.7')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79e43c-7cc2-4933-a771-c169b6c0de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe4a36-aa6c-4e36-b261-f0884fc6d419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a2fb9-2a09-4de8-a6d7-ae46d1efc728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax.scatter(y_test, y_pred,\n",
    "           fc='k', ec='none', lw=1, alpha=0.4,\n",
    "           s=6**2)\n",
    "ax.plot([0,1],[0,1],\n",
    "        color='tab:red', lw=4, alpha=0.8)\n",
    "\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "ax.set_ylabel('Predicted label', fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('True label', fontsize=15, fontweight='bold')\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69821a-3299-4288-8b98-a4320230ed96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

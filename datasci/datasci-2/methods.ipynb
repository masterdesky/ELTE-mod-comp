{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229dacaa-b6d5-4fa1-9ada-985b33c374d9",
   "metadata": {},
   "source": [
    "# Classical methods in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdd4f9-9270-4047-87aa-074f69d3213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Scikit-learn, tensorflow, torch, etc.\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification, \\\n",
    "                             make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1af916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seaborn with custom settings\n",
    "# Facecolor values from S. Conradi @S_Conradi/@profConradi\n",
    "custom_settings = {\n",
    "    'figure.facecolor': '#f4f0e8',\n",
    "    'axes.facecolor': '#f4f0e8',\n",
    "    'axes.edgecolor': '0.7',\n",
    "    'axes.linewidth' : '2',\n",
    "    'grid.color': '0.7',\n",
    "    'grid.linestyle': 'none',\n",
    "    'grid.alpha': 0.6,\n",
    "}\n",
    "sns.set_theme(palette=sns.color_palette('deep', as_cmap=False),\n",
    "              rc=custom_settings)\n",
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbde2dc-b452-4daf-9813-0d31677cde14",
   "metadata": {},
   "source": [
    "# 2. Training (\"fitting\") a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3dc03-aa45-4474-88bf-0935d6ff5dd5",
   "metadata": {},
   "source": [
    "## 2.1. Regression\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Continuous value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e23cb8-b444-437c-88f7-b81eaf790e49",
   "metadata": {},
   "source": [
    "### 2.1.1. Generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16aa33-c1d7-4b0f-b844-a6be1afe954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=5000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    random_state=57\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c2864-2976-4f1e-b2b8-515614b5dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54969415-081a-4944-8132-4a364d8d212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5), dpi=120)\n",
    "\n",
    "ax.plot(y, color='indianred', lw=2)\n",
    "\n",
    "ax.set_title('$y_{\\\\text{values}}$',\n",
    "             fontsize=30, fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74bf12-be90-43c2-86fb-c19867b4f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, \n",
    "               color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdabb6-8bf0-4f6b-aa1b-7acb5459d4f7",
   "metadata": {},
   "source": [
    "#### Pull up the regression methods...\n",
    "\n",
    "There's lots of them...: https://en.wikipedia.org/wiki/Outline_of_machine_learning#Regression_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02d01b-7de8-463f-87d9-d65625118ad0",
   "metadata": {},
   "source": [
    "#### 1. Split the data set to a train and a test set\n",
    "\n",
    "One of the most fundamental principles of machine learning is splitting the dataset into at least two subsets&mdash;a \"training set\" and a \"test set\"&mdash;and, in more advanced workflows, a third subset called a \"validation set.\" The main idea is to fit (or \"train\") the model on the training set, then evaluate its performance on the test set. \n",
    "\n",
    "Why do we do this? When creating abstract models, our primary goal is for them to generalize well to unseen data. A model should not only describe known observations but also predict future, unseen events. In statistics, we \"simulate\" novel observations by designating part of the dataset as \"unseen\"&mdash;that is, the test set. If a trained model performs well on the test set, we can be confident it will generalize effectively to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384efada-b7df-4e9e-a3ae-bf41e7539b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645d346-8578-4e2a-8078-98f5f8d1c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf4713-ea4a-41fb-88ae-e7a6cb1f2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X_train[i], y_train, label='Train set',\n",
    "               color='indianred', s=4**2, alpha=0.6)\n",
    "    ax.scatter(X_test[i], y_test, label='Test set',\n",
    "               color='cornflowerblue', s=4**2, alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab8993-deaf-47a0-b9fa-b0b95c8b408f",
   "metadata": {},
   "source": [
    "#### 2. Train and evaluate some linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224cf5f-3a60-4a47-ab63-6043140d1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression, BayesianRidge, ElasticNet, \\\n",
    "                                 Lasso, LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5927360-66e3-4a41-983a-9f2fd2965adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(model, *, X_train, y_train, X_test, y_test):\n",
    "    reg = model()\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(f\"Score for {model.__name__} : {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879934b-1677-4263-a734-cc258e83e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ARDRegression, BayesianRidge, ElasticNet,\n",
    "    Lasso, LinearRegression, Ridge, SGDRegressor, SVR\n",
    "]\n",
    "for model in models:\n",
    "    regression(model,\n",
    "               X_train=X_train, y_train=y_train,\n",
    "               X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbb3ba-cf96-4556-aa69-42ab6c2d7f07",
   "metadata": {},
   "source": [
    "#### +1. Fun fact: \"regression\" and \"linear regression\" are not necessarily \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704236cf-0256-467a-9142-9f173944fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an oddly specific toy dataset, which 99% of the times are\n",
    "# shown as an example, when this fun fact arises\n",
    "x_1, x_2, x_3, x_4 = 1/4, -3.4, 90, 2\n",
    "X = np.linspace(-40, 40, 500)\n",
    "y_sq = x_1 * X**2\n",
    "y_li = x_2 * X + x_4\n",
    "y_tr = x_3 * np.cos(X)**3\n",
    "y = y_sq + y_li + y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163de01-0c4a-4966-bf4b-b85fe6d9a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=120)\n",
    "\n",
    "ax.grid(True, ls='--', color='.7', alpha=0.4)\n",
    "ax.scatter(X, y, label='Original data',\n",
    "           color='0.4', ec='none', s=7**2, alpha=0.7)\n",
    "title = f'Equation of sampled polynomial: \\n' + \\\n",
    "        f'$({x_1:.3f}\\\\,x^2) + ({x_2:.3f}\\\\,x) + ({x_3:.3f}\\\\,\\\\cos^3(x)) + ({x_4:.3f})$'\n",
    "ax.set_title(title, fontsize=16, fontweight='bold', loc='left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3e00a-3d7a-42db-bf7e-c03935779552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22645d55-846a-4d7e-9743-dd6efd9e12dd",
   "metadata": {},
   "source": [
    "Using `sklearn` we can sequentially transform a dataset and then fit an estimator on it. This sequential list of transformations, finished by a single, final estimator are referred to as a \"pipeline\" in `sklearn`. The pipeline below defined as\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([(\"polynomial_variation\", FunctionTransformer(poly2_reg)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "```\n",
    "\n",
    "contains some arbitrary transformation defined by the `poly2_reg` function (this can be anything actually that transform `X` in any way), which is then fitted using `sklearn`'s built-in linear estimator, the `LinearRegressor`, which implements the ordinary least squares linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92a796-6935-4d6d-86a1-36f0d920bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2_reg(X):\n",
    "    \"\"\"\n",
    "    Returns the transformed array using the equation\n",
    "       ```A * X^2 + B * cos^3(X) + C * X + D```\n",
    "    \"\"\"\n",
    "    return np.hstack((np.cos(X)**3, X, X**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3ec4f-02d9-48ae-8f5e-368cc8d38121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"polynomial_variation\", FunctionTransformer(poly2_reg)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "# Transform X for the PolynomialFeatures() and LinearRegression() class\n",
    "# Then fit on the pipeline the available data\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "# Get coefficients\n",
    "c, b, a = pipeline[1].coef_\n",
    "d = pipeline[1].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coefficients: x_1: {a:.2f}, x_2: {b:.2f}, x_3: {c:.2f}, const.: {d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=120)\n",
    "\n",
    "ax.grid(True, ls='--', color='.7', alpha=0.4)\n",
    "ax.scatter(X, y, label='Original data',\n",
    "           color='0.5', ec='none', s=7**2, alpha=0.7)\n",
    "ax.plot(X, pipeline.predict(X[:, np.newaxis]), label='Fitted model',\n",
    "        color='tab:red', lw=3, ls=(0, (2, 1)), alpha=0.8)\n",
    "\n",
    "title = f'Equation of fitted polynomial: \\n' + \\\n",
    "        f'$({a:.3f}\\\\,x^2) + ({b:.3f}\\\\,x) + ({c:.3f}\\\\,\\\\cos^3(x)) + ({d:.3f})$'\n",
    "ax.set_title(title, fontsize=15, fontweight='bold', loc='left')\n",
    "ax.legend(loc='best', fontsize=15,\n",
    "          facecolor='#f4f0e8', edgecolor='none', framealpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b89de-d60a-4d5a-934e-fb629aae9cfa",
   "metadata": {},
   "source": [
    "### 2.1.2. Finally, some real data...\n",
    "\n",
    "In this section, we will use the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from the UCI Machine Learning Repository, which contains detailed crime statistics from various communities across the US. Using `LinearRegression` and `Lasso` regression models, we will try to identify which features contribute the most to the overall crime rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a797d",
   "metadata": {},
   "source": [
    "#### 0. Load the data set\n",
    "\n",
    "Before we can even load the data set, we first need to understand its structure first. The data is provided as a CSV file named `communities.data`, which we can easily load using `pandas` directly from the UCI archive via a URL. However, the feature names and their descriptions are not included in the CSV file, as they are shipped separately in a text file called `communities.names` along with additional metadata. To construct a proper `DataFrame` from the data itself, we need to load and parse this file to extract the relevant feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a96271-838d-4276-98cf-633480e24fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f51c81-7e35-4e0b-8c32-6ea106767a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names start with `@attribute`, followed by the feature name,\n",
    "# then ending with the type of the feature values (numeric/string/etc.)\n",
    "archive_url = 'https://web.archive.org/web/20230321133656/'\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.names'\n",
    "with request.urlopen(url) as f:\n",
    "    lines = f.read().decode('utf-8').splitlines()\n",
    "features = [re.sub(r'^@attribute\\s+', '', line).split()[0] \n",
    "            for line in lines if line.startswith('@attribute')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be337314-0689-46ea-a72d-51e444d61be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are marked with an `?` in the dataset\n",
    "archive_url = 'https://web.archive.org/web/20240810114503/'\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
    "df = pd.read_csv(url, sep=',', names=features, na_values=['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf601f7-93c5-4166-93b0-bc517c7fa566",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed34d4a-6e5e-42cf-9e88-27378ba821c9",
   "metadata": {},
   "source": [
    "#### 1. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97d5db-7e68-430c-87cb-204a114571fd",
   "metadata": {},
   "source": [
    "#### Handle missing/ID labels\n",
    "\n",
    "While missing values in meaningful features should be filled appropriately, columns representing ID-like variables can be deleted. Location and violent crime rates do correlates in real life, but an idea of a causal relationship between location and crime rates can be discarded now. Description for each feature can be accessed in the `community.names` file.\n",
    "\n",
    "#### ID-like columns\n",
    "The first 4 columns (`state`, `county`, `community`, `communityname`) can be deleted, because they are not meaningful features. The `state` and `county` columns are categorical variables, while the `community` column is a unique identifier for each community. Finally, the `communityname` column is a string representation of the community name, all of which are not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49d099-9270-45f6-aab5-4fcc7828d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be1ed8-0c29-475a-9b45-7fee4abbf6e6",
   "metadata": {},
   "source": [
    "The column `fold` is a debug feature, which is just a remnant from a cross-validation applied during creation of the data set; this can be also discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055a8bc-b261-4516-bebb-21cc89325209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430571af-b1c2-4f64-8b34-2a1fed798bd7",
   "metadata": {},
   "source": [
    "#### Features with missing values\n",
    "\n",
    "According to the feature descriptions, all remaining columns are in a decimal format and scaled into the interval of $\\left[ 0, 1 \\right]$. The only exception is the feature `LemasGangUnitDeploy`, which is actually an ordinal with values $0.0$, $0.5$ and $1.0$. We can still however handle it as a decimal feature.\n",
    "\n",
    "There is a table in the `community.names` metadata file which summarizes the basic statistical attributes (mean, median, standard deviation, etc.) of each feature in the dataset. According to this table, any feature with missing entries has exactly $1675$ missing values. (There is only one exception: the column `OtherPerCap`, where only $1$ value is missing.) It is entirely logical to assume that, in this case, the missing features are always missing from the same lines. If this hypothesis is true, we can test it by visualizing the missing values on a matrix plot. If we plot features on the $y$-axis, we should see only horizontal lines (interrupted by vertical gaps) in the dataset, instead of individual points scattered throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9288f21-e30e-4cb0-995f-6567b2ccf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30), dpi=300)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.imshow(df.isna().T, interpolation='none')\n",
    "ax.set_xlabel('Rows', fontweight='bold')\n",
    "ax.set_ylabel('Features', fontweight='bold')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68fee4-848d-450c-91cc-c208bcc2f05a",
   "metadata": {},
   "source": [
    "These are indeed \"horizontal lines interrupted by vertical gaps.\" However, these features are missing most of their values. In this case, we should consider simply dropping these features from the model, since filling them with artificial values could reasonably distort their impact on the model. I will try this method for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b66b42-f115-412f-b47c-c8e198c67cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with at least 50% of values missing\n",
    "df_n = df.dropna(axis=1, thresh=int(0.5 * len(df)), inplace=False)\n",
    "\n",
    "# Fill that 1 remaining entry with the mean of the corresponding feature\n",
    "df_n = df_n.fillna(df_n.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d199c-d838-4846-b25d-e31fd2680120",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30), dpi=300)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.imshow(df_n.isna().T, interpolation='none')\n",
    "ax.set_xlabel('Rows', fontweight='bold')\n",
    "ax.set_ylabel('Features', fontweight='bold')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cb3e0-69c0-473c-9874-06c1fad551ba",
   "metadata": {},
   "source": [
    "#### Scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031264a-5d15-4896-8afa-04620c4434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y datasets\n",
    "X = df_n[df_n.columns[:-1]]\n",
    "y = df_n[df_n.columns[-1]]\n",
    "# Scale the dataset\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88217e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2), dpi=200)\n",
    "\n",
    "ax.plot(y, color='indianred', lw=2)\n",
    "ax.set_title('Violent crime rate per pop (normalized to [0, 1])', loc='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df94ff8-abd0-421c-93c3-a409b87f18a9",
   "metadata": {},
   "source": [
    "#### 2. Fit linear regression using 5-fold CV\n",
    "\n",
    "<img width=\"800px\" src=\"./images/5foldcv.png\" style=\"display:block; margin:auto;\"/>\n",
    "\n",
    "5-fold cross-validation is a widely utilized technique used to assess the performance of a machine learning model. It helps to estimate how well a model generalizes to unseen data by repeatedly training and testing on different subsets of the data. The process involves the following steps:\n",
    "\n",
    "1. **Split** the dataset into 5 equal \"folds\" (subsets).  \n",
    "2. **Loop** over the 5 folds:\n",
    "   - Use 4 folds for **training**.\n",
    "   - Use the remaining 1 fold for **validation/testing**.\n",
    "3. **Record** the validation error (e.g. MSE) for each fold.\n",
    "4. **Average** the 5 validation errors, which will provide a robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f4a55-ee0e-4fad-bf57-83c377ec38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba327e4b-f7b2-4f08-b3d9-91cc8aaafb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b867cd-c54a-424c-8767-57b27bc27c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2 score\n",
    "# Refrence: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv)\n",
    "\n",
    "print('KFOLD SCORES:\\n' +\n",
    "      '----------------')\n",
    "print(scores)\n",
    "print('Mean of scores : {0:.4f}'.format(np.mean(scores)))\n",
    "print('Std of scores : {0:.4f}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a0487-b7b3-4341-801b-fa6de32246cc",
   "metadata": {},
   "source": [
    "#### 3. Fit Lasso regression using 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7b69d-b6e8-49b7-8add-b34767a7c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb345af6-ad81-4255-bf0c-b5a0e9bbb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just part of the full dataset for training with 5-fold CV\n",
    "# Use the remaining values as a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a55cc-4cb6-48c6-9c1a-172cb9875053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Lasso estimator with scaling the data\n",
    "model = make_pipeline(StandardScaler(), Lasso(random_state=None))\n",
    "# Parameters to explore:\n",
    "# alpha and max_iter\n",
    "param_grid = {\n",
    "    'lasso__alpha' : np.logspace(-5, -1, 50),\n",
    "    'lasso__max_iter' : np.linspace(10, 1000, 10, dtype=int)\n",
    "}\n",
    "# Grid search cross-validation\n",
    "clf = GridSearchCV(estimator=model,\n",
    "                   param_grid=param_grid,\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8a5d1-ab7f-4f52-a6d0-205ac7f15df2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525e265-25e2-48fc-929d-9c69b0ea9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best model : {0}'.format(best_model.named_steps['lasso']))\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Score for Lasso : {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5e8a3-9e2b-47c2-b088-8c433820d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(5, 5), dpi=120)\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color='red', lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "axes.scatter(y_test, y_pred,\n",
    "             color='0.4', s=12**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title('Predictions using the optimized\\n5-fold Lasso regression',\n",
    "               loc='left')\n",
    "axes.set_xlabel('$\\\\mathrm{y_{groundtruth}}$', fontsize=20)\n",
    "axes.set_ylabel('$\\\\mathrm{y_{inferred}}$', fontsize=20)\n",
    "axes.text(0.04, 0.96, f'R$^{{2}}$ score : {r2_score(y_test, y_pred):.4f}',\n",
    "          va='top', ha='left', transform=axes.transAxes,\n",
    "          bbox=dict(boxstyle='square,pad=0.3', fc='none', ec='black', lw=1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e31bca-039b-465a-a12d-5388f67e9fbf",
   "metadata": {},
   "source": [
    "#### Comparing various models like previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7845b4e-537b-44bc-beaf-96302f5ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ARDRegression, BayesianRidge, ElasticNet,\n",
    "    Lasso, LinearRegression, Ridge, SGDRegressor, SVR\n",
    "]\n",
    "for model in models:\n",
    "    regression(model,\n",
    "               X_train=X_train, y_train=y_train,\n",
    "               X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee578316-ec9d-4173-bd93-ed4e59c25dc5",
   "metadata": {},
   "source": [
    "#### Notes on the results\n",
    "\n",
    "The grid search returned a very small (almost the smallest) alpha value in the analysis above. This wasn't actually an error, but the indication, that a linear regression could be efficiently used in case of this specific dataset. ($\\alpha \\to 0$ is equivalent to the linear regression in the case of the Lasso regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb31ed-6fc3-4493-bdfc-9817282547c6",
   "metadata": {},
   "source": [
    "#### 4. Evaluating the trained Lasso model with the shrinkage method\n",
    "\n",
    "The shrinkage method is a \"numerical Occam's razor\", which helps simplify models by discouraging overly complex solutions, typically through penalizing large coefficients. This approach makes models more interpretable, robust, and reduces the risk of overfitting.\n",
    "\n",
    "One popular shrinkage technique is **Lasso Regression** (Least Absolute Shrinkage and Selection Operator). Lasso modifies ordinary least squares regression by adding a penalty term to its loss function, proportional to the absolute values of the regression coefficients. Specifically, the Lasso regression loss function can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\beta)\n",
    "    =\n",
    "    \\underbrace{\\frac{1}{2n} \\sum_{i=1}^{n} (y_{i} - \\mathbf{x}_{i}^{T} \\beta)^{2}}_{\\text{Ordinary Least Squares (OLS)}}\n",
    "    +\n",
    "    \\underbrace{\\alpha \\sum_{j=1}^{p}|\\beta_j|}_{\\text{Lasso penalty term}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\mathbf{x}_i$ is the feature vector for observation $i$.\n",
    "- $y_i$ is the actual outcome for observation $i$.\n",
    "- $\\beta_j$ represents the coefficients (weights) associated with each feature.\n",
    "- $\\alpha$ is a hyperparameter that controls the strength of regularization:\n",
    "    - Larger $\\alpha$ values impose stronger penalties, forcing more coefficients to exactly zero.\n",
    "    - Smaller $\\alpha$ values relax this penalty, allowing more coefficients to remain non-zero.\n",
    "\n",
    "Because of the absolute value penalty $|\\beta_j|$, Lasso regression can shrink coefficients exactly to zero. This property makes it particularly effective for **feature selection**, automatically identifying the most important variables in the dataset by eliminating irrelevant or redundant ones.\n",
    "\n",
    "Essentially, the shrinkage method is the analysis of the coefficients of the trained model as a function of the regularization parameter $\\alpha$. The coefficients are plotted against the $\\log_{10}(\\alpha)$ values, which allows us to visualize how the coefficients change as we vary the strength of the penalty. This plot is often referred to as a \"coefficient path\" or \"regularization path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88255ee8-54e8-4568-acc7-171dadf0283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lasso(X, y, alpha=1.0, max_iter=1e5):\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Lasso(alpha=alpha, max_iter=max_iter, random_state=None))\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e59186-7440-4817-933c-a97dcc0c62db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_alphas = np.logspace(-5, 1, 100)\n",
    "lasso_coeffs = np.zeros((len(lasso_alphas), X.shape[1]))\n",
    "\n",
    "for i, a in enumerate(lasso_alphas):\n",
    "    model = evaluate_lasso(X_train, y_train, alpha=a, max_iter=300)\n",
    "    lasso_coeffs[i] = model.named_steps['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3382a7-b28d-4880-8b88-c500bc474cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 2\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*10, nr*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(lasso_alphas.min()), np.log(lasso_alphas.max()))\n",
    "ax.set_title('Full test range',\n",
    "             fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-6, -1)\n",
    "ax.set_ylim(-0.08, 0.08)\n",
    "ax.set_title('Zoomed on interesting area',\n",
    "             fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(lasso_alphas), lasso_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel('$\\\\log \\\\left( \\\\alpha \\\\right)$',\n",
    "                  fontsize=16, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients',\n",
    "                  fontsize=16, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   labelsize=12, colors='white', rotation=20)\n",
    "\n",
    "fig.suptitle('Shrinkage method used on the results of Lasso regression.',\n",
    "             color='white', fontsize=21, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca8128-08b6-4ff5-b62d-23742f45e9cb",
   "metadata": {},
   "source": [
    "Around $\\log(\\alpha) \\approx -5$ is where mostly the interesting events happen. That is the range, where a lot of coefficients diverges away from 0, while other coefficients vanish. Two other coefficients does the same, but with much a much smaller extent around $\\log(\\alpha) \\approx -2$, before vanishing quickly. Let's see which features are responsible for this last anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47242aa-7c77-4f9e-88ba-a11dccbc5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(35, 35))\n",
    "\n",
    "ax.imshow(lasso_coeffs.T, aspect=0.8, cmap='seismic')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "ax.set_yticks([i for i in range(len(df_n.columns[:-1]))])\n",
    "ax.set_yticklabels(df_n.columns.tolist()[:-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eeb6f-a66d-4d9a-bdf5-5aa5571c0208",
   "metadata": {},
   "source": [
    "## 2.2. Clustering (#3 in the `problems.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17646e43-f4f6-4cde-9d22-13e71f159080",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    n_features=2,   # Dimension of the dataset (Here it's a 2D dataset)\n",
    "    centers=3,      # Number of blobs to create\n",
    "    cluster_std=[1.0, 2.5, 0.5],\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of circles\n",
    "Xc, yc = make_circles(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    noise=0.05,\n",
    "    factor=0.5,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of moons\n",
    "Xm, ym = make_moons(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    noise=0.1,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468202ca-86b5-4375-aeb9-b7950a9138b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize them\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(8*nc, 8*nr))\n",
    "\n",
    "Xi = (Xb, Xc, Xm)\n",
    "yi = (yb, yc, ym)\n",
    "for X, y, ax in zip(Xi, yi, axes.flat):\n",
    "\n",
    "    X = X - np.mean(X)\n",
    "    ax.scatter(*X.T, c=cm.viridis(y/y.max()))\n",
    "\n",
    "    lim = 1.1 * np.max(np.abs(X))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8528ff-8ab4-48da-9433-9e98aa8c30cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### An example for clustering: naive *k*-means algorithm\n",
    "\n",
    "<img src=\"./images/kmeans.gif\" style=\"display:block; margin:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d2fa-2d2a-40f6-bec8-1217e974010e",
   "metadata": {},
   "source": [
    "### Compare different types of clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb74bc2-e0dd-4b7e-8d69-0a0b17189e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from itertools import cycle, islice\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import MeanShift, MiniBatchKMeans, AffinityPropagation, \\\n",
    "                            AgglomerativeClustering, SpectralClustering, \\\n",
    "                            DBSCAN, OPTICS, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2fc61-e67e-4562-bfc9-444bbb9cf41f",
   "metadata": {},
   "source": [
    "### Define the datasets and corresponding clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f29a56-936e-456d-b1e4-a97cf52d8286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "  (\n",
    "    Xb, {}\n",
    "  ),\n",
    "  (\n",
    "    Xc, {\n",
    "      \"damping\": 0.77,\n",
    "      \"preference\": -240,\n",
    "      \"quantile\": 0.2,\n",
    "      \"min_samples\": 20,\n",
    "      \"xi\": 0.25,\n",
    "      \"n_clusters\": 2,\n",
    "    }\n",
    "  ),\n",
    "  (\n",
    "    Xm, {\n",
    "      \"damping\": 0.75,\n",
    "      \"preference\": -220,\n",
    "      \"n_clusters\": 2\n",
    "    }\n",
    "  )\n",
    "]\n",
    "\n",
    "default_params = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 20,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6825f-c2e2-46bb-bf31-1941091f14d0",
   "metadata": {},
   "source": [
    "### Define different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3b4c9-051c-45f6-974c-57cdde247923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_clustering_algos(params, **kwargs):\n",
    "\n",
    "    bandwidth = None if not kwargs else kwargs[\"bandwidth\"]\n",
    "    connectivity = None if not kwargs else kwargs[\"connectivity\"]\n",
    "\n",
    "    ms = MeanShift(\n",
    "        bandwidth=bandwidth,\n",
    "        bin_seeding=True\n",
    "    )\n",
    "    two_means = MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"]\n",
    "    )\n",
    "    affinity_propagation = AffinityPropagation(\n",
    "        damping=params[\"damping\"],\n",
    "        preference=params[\"preference\"],\n",
    "        random_state=0\n",
    "    )\n",
    "    ward = AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        linkage=\"ward\",\n",
    "        connectivity=connectivity\n",
    "    )\n",
    "    average_linkage = AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        metric=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        connectivity=connectivity,\n",
    "    )\n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    )\n",
    "    dbscan = DBSCAN(\n",
    "        eps=params[\"eps\"]\n",
    "    )\n",
    "    optics = OPTICS(\n",
    "        min_samples=params[\"min_samples\"],\n",
    "        xi=params[\"xi\"],\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "    )\n",
    "    birch = Birch(\n",
    "        n_clusters=params[\"n_clusters\"]\n",
    "    )\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"],\n",
    "        covariance_type=\"full\"\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"MeanShift\", ms),\n",
    "        (\"MiniBatch\\nKMeans\", two_means),\n",
    "        (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        (\"DBSCAN\", dbscan),\n",
    "        (\"OPTICS\", optics),\n",
    "        (\"BIRCH\", birch),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )\n",
    "\n",
    "    return clustering_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517f0c5-3fb9-4863-ab36-1ce1a524ea52",
   "metadata": {},
   "source": [
    "#### Stolen and reworked from matplotlib's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d1146-7ad1-4bfc-82da-55c30e66ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "nr, nc = len(datasets), len(return_clustering_algos(default_params))\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(4*nc, 4*nr))\n",
    "fig.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "\n",
    "for dataset_i, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_params.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    #X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    extra_params = {\"bandwidth\" : bandwidth, \"connectivity\" : connectivity}\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    clustering_algorithms = return_clustering_algos(params, **extra_params)\n",
    "\n",
    "    for ax_i, (name, algorithm) in enumerate(clustering_algorithms):\n",
    "        # Fit\n",
    "        t0 = time.time()\n",
    "        algorithm.fit(X)\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        ax = axes[dataset_i, ax_i]\n",
    "        ax.axis('off')\n",
    "        if dataset_i == 0:\n",
    "            ax.set_title(name, fontsize=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        ax.scatter(*X.T, color=np.array(colors)[y_pred], s=16)\n",
    "\n",
    "        lim = 1.1 * np.max(np.abs(X))\n",
    "        ax.set_xlim(-lim, lim)\n",
    "        ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f018726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac73cad0-7ea9-43df-a4c9-7c481d5926ab",
   "metadata": {},
   "source": [
    "## 3. Classification (#2 in the `problems.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e027278-6b7c-430c-b152-318f24970187",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=100,    # Number of points in the data set\n",
    "    n_features=6,     # Number of features in the data set\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0998e-ca46-4ffb-823c-ed7e5feab786",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3db72-e654-4703-876e-9b3b0db9681f",
   "metadata": {},
   "source": [
    "### Literally the same as regression, but with different methods..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

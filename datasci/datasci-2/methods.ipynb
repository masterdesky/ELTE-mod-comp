{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229dacaa-b6d5-4fa1-9ada-985b33c374d9",
   "metadata": {},
   "source": [
    "# Data science in Python II. - Classical machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdd4f9-9270-4047-87aa-074f69d3213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Scikit-learn, tensorflow, torch, etc.\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification, \\\n",
    "                             make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbde2dc-b452-4daf-9813-0d31677cde14",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3dc03-aa45-4474-88bf-0935d6ff5dd5",
   "metadata": {},
   "source": [
    "## 2.1. Regression\n",
    "\n",
    "<p style=\"text-align:center; font-size:20px;\">\n",
    "  <b>Data and label -> Model -> Continuous value</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e23cb8-b444-437c-88f7-b81eaf790e49",
   "metadata": {},
   "source": [
    "### 2.1.1. Generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16aa33-c1d7-4b0f-b844-a6be1afe954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=5000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    random_state=57\n",
    ")\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c2864-2976-4f1e-b2b8-515614b5dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54969415-081a-4944-8132-4a364d8d212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5), dpi=120)\n",
    "ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "ax.plot(y, lw=2)\n",
    "\n",
    "ax.set_title(\"\\\\textbf{y values}\",\n",
    "             fontsize=30, fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74bf12-be90-43c2-86fb-c19867b4f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[i], y, \n",
    "               color='indianred', alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdabb6-8bf0-4f6b-aa1b-7acb5459d4f7",
   "metadata": {},
   "source": [
    "### Pull up the regression methods...\n",
    "\n",
    "There's lots of them...: https://en.wikipedia.org/wiki/Outline_of_machine_learning#Regression_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02d01b-7de8-463f-87d9-d65625118ad0",
   "metadata": {},
   "source": [
    "#### 1. Split the dataset to a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384efada-b7df-4e9e-a3ae-bf41e7539b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645d346-8578-4e2a-8078-98f5f8d1c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf4713-ea4a-41fb-88ae-e7a6cb1f2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 5\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*5, nr*5), dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X_train[i], y_train, label='Train set',\n",
    "               color='indianred', s=4**2, alpha=0.6)\n",
    "    ax.scatter(X_test[i], y_test, label='Test set',\n",
    "               color='cornflowerblue', s=4**2, alpha=0.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(f'$X_{{{i+1}}}$', fontsize=30, fontweight='bold')\n",
    "    ax.set_ylabel('$y$', fontsize=30, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab8993-deaf-47a0-b9fa-b0b95c8b408f",
   "metadata": {},
   "source": [
    "#### 2. Train and evaluate some linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224cf5f-3a60-4a47-ab63-6043140d1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression, BayesianRidge, ElasticNet, \\\n",
    "                                 Lasso, LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5927360-66e3-4a41-983a-9f2fd2965adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(model, *, X_train, y_train, X_test, y_test):\n",
    "    reg = model()\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(f\"Score for {model.__name__} : {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879934b-1677-4263-a734-cc258e83e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ARDRegression, BayesianRidge, ElasticNet,\n",
    "    Lasso, LinearRegression, Ridge, SGDRegressor, SVR\n",
    "]\n",
    "for model in models:\n",
    "    regression(model,\n",
    "               X_train=X_train, y_train=y_train,\n",
    "               X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbb3ba-cf96-4556-aa69-42ab6c2d7f07",
   "metadata": {},
   "source": [
    "### Fun fact: \"Regression\" and \"linear regression\" are not necessarily \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704236cf-0256-467a-9142-9f173944fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an oddly specific toy dataset, which 99% of the times are\n",
    "# shown as an example, when this fun fact arises.\n",
    "X = np.linspace(-40, 40, 500)\n",
    "y_sq = 1/4 * X**2\n",
    "y_li = -3.4 * X + 2\n",
    "y_tr = 90 * np.cos(X)**3\n",
    "y = y_sq + y_li + y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163de01-0c4a-4966-bf4b-b85fe6d9a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6),\n",
    "                       facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "ax.grid(True, ls='--', color='.7', alpha=0.4)\n",
    "ax.scatter(X, y, label='Original data',\n",
    "           color=cm.magma(y/y.max()/2 + 0.5), ec='none', s=7**2, alpha=0.7)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16, colors='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3e00a-3d7a-42db-bf7e-c03935779552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22645d55-846a-4d7e-9743-dd6efd9e12dd",
   "metadata": {},
   "source": [
    "Using `sklearn` we can sequentially transform a dataset and then fit an estimator on it. This sequential list of transformations, finished by a single, final estimator are referred to as a \"pipeline\" in `sklearn`. The pipeline below defined as\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([(\"polynomial_variation\", FunctionTransformer(poly2_reg)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "```\n",
    "\n",
    "contains some arbitrary transformation defined by the `poly2_reg` function (this can be anything actually that transform `X` in any way), which is then fitted using `sklearn`'s built-in linear estimator, the `LinearRegressor`, which implements the ordinary least squares linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92a796-6935-4d6d-86a1-36f0d920bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2_reg(X):\n",
    "    \"\"\"\n",
    "    Returns the transformed array using the equation\n",
    "       ```A * X^2 + B * cos^3(X) + C * X + D```\n",
    "    \"\"\"\n",
    "    return np.hstack((np.cos(X)**3, X, X**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3ec4f-02d9-48ae-8f5e-368cc8d38121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"polynomial_variation\", FunctionTransformer(poly2_reg)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "# Transform X for the PolynomialFeatures() and LinearRegression() class\n",
    "# Then fit on the pipeline the available data\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "# Get coefficients\n",
    "c, b, a = pipeline[1].coef_\n",
    "d = pipeline[1].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062916ba-094f-4822-93d8-4ad74849259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6),\n",
    "                       facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "ax.grid(True, ls='--', color='.7', alpha=0.4)\n",
    "ax.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), ec='none', s=7**2, alpha=0.7)\n",
    "ax.plot(X, pipeline.predict(X[:, np.newaxis]), label='Fitted model',\n",
    "        color='tab:green', lw=4, ls='--', alpha=0.8)\n",
    "\n",
    "title = f'Equation of fitted polynomial: \\n' + \\\n",
    "        f'$({a:.3f}\\,x^2) + ({b:.3f}\\,x) + ({c:.3f}\\,\\cos^3(x)) + ({d:.3f})$'\n",
    "ax.set_title(title, fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=16, colors='white')\n",
    "\n",
    "ax.legend(loc='best', fontsize=24,\n",
    "          facecolor='black', labelcolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b89de-d60a-4d5a-934e-fb629aae9cfa",
   "metadata": {},
   "source": [
    "## 2.1.2. Use of some real data finally...\n",
    "\n",
    "Here we're using the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from UCI, which contains detailed crime statistics from the US. Using `LinearRegression` and the `Lasso` regression models, we're trying to find which feature attributes the most to the crime rate in the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a96271-838d-4276-98cf-633480e24fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f51c81-7e35-4e0b-8c32-6ea106767a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "# Feature names start with `@attribute`, followed by the feature name,\n",
    "# then ending with the type of the feature values (numeric/string/etc.)\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.names'\n",
    "with request.urlopen(url) as f:\n",
    "    while True:\n",
    "        line = f.readline().decode(\"utf-8\")\n",
    "        if not line:\n",
    "            break\n",
    "        if '@attribute' in line:\n",
    "            features.append(line.strip().split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be337314-0689-46ea-a72d-51e444d61be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are marked with an `?` in the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
    "df = pd.read_csv(url, sep=',', names=features, na_values=['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf601f7-93c5-4166-93b0-bc517c7fa566",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed34d4a-6e5e-42cf-9e88-27378ba821c9",
   "metadata": {},
   "source": [
    "### 2.1.2./a. Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97d5db-7e68-430c-87cb-204a114571fd",
   "metadata": {},
   "source": [
    "#### Handle missing/ID labels\n",
    "\n",
    "While missing values in meaningful features should be filled appropriately, columns representing ID-like variables can be deleted. Location and violent crime rates do correlates in real life, but an idea of a causal relationship between location and crime rates can be discarded now. Description for each feature can be accessed in the `community.names` file.\n",
    "\n",
    "#### ID-like columns\n",
    "The first 4 columns (`state`, `county`, `community`, `communityname`) can be deleted, because they represent locational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49d099-9270-45f6-aab5-4fcc7828d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be1ed8-0c29-475a-9b45-7fee4abbf6e6",
   "metadata": {},
   "source": [
    "The column `fold` is a debug feature from cross-validation, which can be also discarded for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055a8bc-b261-4516-bebb-21cc89325209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430571af-b1c2-4f64-8b34-2a1fed798bd7",
   "metadata": {},
   "source": [
    "#### Features with missing values\n",
    "\n",
    "According to the feature descriptions, all remaining columns are in a decimal format and scaled into the interval of $\\left[ 0, 1 \\right]$. The only exception is the feature `LemasGangUnitDeploy`, which is actually an ordinal with values $0.0$, $0.5$ and $1.0$. We can still however handle it as a decimal feature.\n",
    "\n",
    "There is a table in the `community.names` description file, which summarize the basic statistical attributes (mean, median, standard deviation, etc.) of each features in the dataset. According to this table any features with missing entries have exactly $1675$ missing values in each of them. (There is only one exception, the column `OtherPerCap`, where only $1$ value is missing.). It is entirely logical to assume that in this case the missing features are always missing from the same lines. If the hypothesis is true, we can test it by visualizing the missing values on a matrixplot. If we plot features on the $y$-axis, we should see only horizontal lines (which are interrupted by vertical gaps) in the dataset, instead of individual points scattered all around the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9288f21-e30e-4cb0-995f-6567b2ccf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 100\n",
    "fig, axes = plt.subplots(figsize=(figsize,figsize))\n",
    "axes.set_aspect('equal')\n",
    "axes.grid(False)\n",
    "\n",
    "axes.imshow(df.isna().T, interpolation='none')\n",
    "axes.set_xlabel('Locations', fontsize=50, fontweight='bold')\n",
    "axes.set_ylabel('Features', fontsize=50, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68fee4-848d-450c-91cc-c208bcc2f05a",
   "metadata": {},
   "source": [
    "These are indeed \"horizontal lines interrupted by vertical gaps\". However these features miss most of their values. In this case we should consider simply dropping these features from the model, since filling them up with artificial values could reasonably distort the impact of these features on our model. I'll try this method for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b66b42-f115-412f-b47c-c8e198c67cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with at least 50% of values missing\n",
    "df_n = df.dropna(axis=1, thresh=int(0.5 * len(df)), inplace=False)\n",
    "\n",
    "# Fill that 1 remaining entry with the mean of the corresponding feature\n",
    "df_n = df_n.fillna(df_n.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d199c-d838-4846-b25d-e31fd2680120",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 100\n",
    "fig, axes = plt.subplots(figsize=(figsize,figsize))\n",
    "axes.set_aspect('equal')\n",
    "axes.grid(False)\n",
    "\n",
    "axes.imshow(df_n.isna().T)\n",
    "axes.set_xlabel('Locations', fontsize=50, fontweight='bold')\n",
    "axes.set_ylabel('Features', fontsize=50, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cb3e0-69c0-473c-9874-06c1fad551ba",
   "metadata": {},
   "source": [
    "#### Scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031264a-5d15-4896-8afa-04620c4434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y datasets\n",
    "X = df_n[df_n.columns[:-1]]\n",
    "y = df_n[df_n.columns[-1]]\n",
    "# Scale the dataset\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df94ff8-abd0-421c-93c3-a409b87f18a9",
   "metadata": {},
   "source": [
    "### 2.1.2./b. Fit linear regression using 5-fold CV\n",
    "\n",
    "<img width=\"800px\" src=\"./images/5foldcv.png\" style=\"display:block; margin:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f4a55-ee0e-4fad-bf57-83c377ec38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba327e4b-f7b2-4f08-b3d9-91cc8aaafb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b867cd-c54a-424c-8767-57b27bc27c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2 score\n",
    "# Refrence: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv)\n",
    "\n",
    "print('KFOLD SCORES:\\n' +\n",
    "      '----------------')\n",
    "print(scores)\n",
    "print('Mean of scores : {0:.4f}'.format(np.mean(scores)))\n",
    "print('Std of scores : {0:.4f}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a0487-b7b3-4341-801b-fa6de32246cc",
   "metadata": {},
   "source": [
    "### 2.1.2./d. Fit Lasso regression using 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7b69d-b6e8-49b7-8add-b34767a7c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb345af6-ad81-4255-bf0c-b5a0e9bbb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just part of the full dataset for training with 5-fold CV\n",
    "# Use the remaining values as a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a55cc-4cb6-48c6-9c1a-172cb9875053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Lasso estimator\n",
    "model = Lasso(random_state=None)\n",
    "# Paramters to explored:\n",
    "# alpha, normalize, max_iter\n",
    "param_grid = {\n",
    "    'alpha' : np.logspace(-10, 0.1, 20),\n",
    "    'max_iter' : np.logspace(4, 6, 10, dtype=int)\n",
    "}\n",
    "# Grid search cross-validation\n",
    "clf = GridSearchCV(estimator=model,\n",
    "                   param_grid=param_grid,\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8a5d1-ab7f-4f52-a6d0-205ac7f15df2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525e265-25e2-48fc-929d-9c69b0ea9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best model : {0}'.format(best_model))\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Score for Lasso : {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5e8a3-9e2b-47c2-b088-8c433820d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(5, 5), dpi=120,\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "axes.scatter(y_test, y_pred,\n",
    "             color=cm.magma(0.5), s=12**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title(f'R$^{2}$ score : {r2_score(y_test, y_pred):.4f}',\n",
    "               fontsize=16, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$\\mathrm{y_{groundtruth}}$',\n",
    "                fontsize=30, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$\\mathrm{y_{pred}}$',\n",
    "                fontsize=30, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major',\n",
    "                 labelsize=20, colors='white', rotation=20)\n",
    "\n",
    "fig.suptitle('Predictions made with the optimized\\n5-fold Lasso regression.',\n",
    "             color='white', fontsize=21, y=-0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e31bca-039b-465a-a12d-5388f67e9fbf",
   "metadata": {},
   "source": [
    "#### Using the models as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7845b4e-537b-44bc-beaf-96302f5ed0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ARDRegression, BayesianRidge, ElasticNet,\n",
    "    Lasso, LinearRegression, Ridge, SGDRegressor, SVR\n",
    "]\n",
    "for model in models:\n",
    "    regression(model,\n",
    "               X_train=X_train, y_train=y_train,\n",
    "               X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee578316-ec9d-4173-bd93-ed4e59c25dc5",
   "metadata": {},
   "source": [
    "### Notes on the results in 2.1.2./d.\n",
    "\n",
    "The grid search returned a very small (almost the smallest) alpha value in the analysis above. This wasn't actually an error, but the indication, that a linear regression could be efficiently used in case of this specific dataset. ($\\alpha \\to 0$ is equivalent to the linear regression in the case of the Lasso regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb31ed-6fc3-4493-bdfc-9817282547c6",
   "metadata": {},
   "source": [
    "### 2.1.2./e. Lasso evaluation with shrinkage method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88255ee8-54e8-4568-acc7-171dadf0283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lasso(X, y, alpha=1.0, normalize=False, max_iter=1e5):\n",
    "\n",
    "    model = Lasso(alpha=alpha, max_iter=max_iter, random_state=None)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e59186-7440-4817-933c-a97dcc0c62db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_alphas = []\n",
    "lasso_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 1, 100):\n",
    "    lasso_alphas.append(a)\n",
    "    model = evaluate_lasso(X_train, y_train, alpha=a, normalize=False, max_iter=10000)\n",
    "    lasso_coeffs.append(model.coef_)\n",
    "\n",
    "lasso_alphas = np.array(lasso_alphas)\n",
    "lasso_coeffs = np.array(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3382a7-b28d-4880-8b88-c500bc474cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 1, 2\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(nc*10, nr*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(lasso_alphas.min()), np.log(lasso_alphas.max()))\n",
    "ax.set_title('Full test range',\n",
    "             fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-6, -1)\n",
    "ax.set_ylim(-0.08, 0.08)\n",
    "ax.set_title('Zoomed on interesting area',\n",
    "             fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(lasso_alphas), lasso_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$',\n",
    "                  fontsize=16, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients',\n",
    "                  fontsize=16, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   labelsize=12, colors='white', rotation=20)\n",
    "\n",
    "fig.suptitle('Shrinkage method used on the results of Lasso regression.',\n",
    "             color='white', fontsize=21, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca8128-08b6-4ff5-b62d-23742f45e9cb",
   "metadata": {},
   "source": [
    "Around $\\log \\left( \\alpha \\right)\\approx-5$ is where mostly the interesting events happen. That's the place, where a lot of coefficients diverges away from 0, while other coefficients vanish. Two other coefficients does the same, but with much a much smaller extent around $\\log \\left( \\alpha \\right) \\approx -12$, before vanishing quickly. Let's see which features are responsible for this last anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47242aa-7c77-4f9e-88ba-a11dccbc5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(35, 35))\n",
    "\n",
    "ax.imshow(lasso_coeffs.T, aspect=0.8, cmap='magma')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "ax.set_yticks([i for i in range(len(df_n.columns[:-1]))])\n",
    "ax.set_yticklabels(df_n.columns.tolist()[:-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eeb6f-a66d-4d9a-bdf5-5aa5571c0208",
   "metadata": {},
   "source": [
    "## 2.2. Clustering (#3 in the `problems.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17646e43-f4f6-4cde-9d22-13e71f159080",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500\n",
    "# Create a dummy dataset of blobs\n",
    "Xb, yb = make_blobs(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    n_features=2,   # Dimension of the dataset (Here it's a 2D dataset)\n",
    "    centers=3,      # Number of blobs to create\n",
    "    cluster_std=[1.0, 2.5, 0.5],\n",
    "    center_box=(-10, 10),\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of circles\n",
    "Xc, yc = make_circles(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    noise=0.05,\n",
    "    factor=0.5,\n",
    "    random_state=57\n",
    ")\n",
    "\n",
    "# Create a dummy dataset of moons\n",
    "Xm, ym = make_moons(\n",
    "    n_samples=N,    # Number of points in the dataset\n",
    "    noise=0.1,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468202ca-86b5-4375-aeb9-b7950a9138b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize them\n",
    "nr, nc = 1, 3\n",
    "fig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(8*nc, 8*nr))\n",
    "\n",
    "Xi = (Xb, Xc, Xm)\n",
    "yi = (yb, yc, ym)\n",
    "for X, y, ax in zip(Xi, yi, axes.flat):\n",
    "    ax.grid(True, ls='--', alpha=0.6)\n",
    "\n",
    "    X = X - np.mean(X)\n",
    "    ax.scatter(*X.T, c=y)\n",
    "\n",
    "    lim = 1.1 * np.max(np.abs(X))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8528ff-8ab4-48da-9433-9e98aa8c30cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### An example for clustering: naive *k*-means algorithm\n",
    "\n",
    "<img src=\"./images/kmeans.gif\" style=\"display:block; margin:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d2fa-2d2a-40f6-bec8-1217e974010e",
   "metadata": {},
   "source": [
    "### Compare different types of clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb74bc2-e0dd-4b7e-8d69-0a0b17189e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from itertools import cycle, islice\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.cluster import MeanShift, MiniBatchKMeans, AffinityPropagation, \\\n",
    "                            AgglomerativeClustering, SpectralClustering, \\\n",
    "                            DBSCAN, OPTICS, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2fc61-e67e-4562-bfc9-444bbb9cf41f",
   "metadata": {},
   "source": [
    "### Define the datasets and corresponding clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f29a56-936e-456d-b1e4-a97cf52d8286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "  (\n",
    "    Xb, {}\n",
    "  ),\n",
    "  (\n",
    "    Xc, {\n",
    "      \"damping\": 0.77,\n",
    "      \"preference\": -240,\n",
    "      \"quantile\": 0.2,\n",
    "      \"min_samples\": 20,\n",
    "      \"xi\": 0.25,\n",
    "      \"n_clusters\": 2,\n",
    "    }\n",
    "  ),\n",
    "  (\n",
    "    Xm, {\n",
    "      \"damping\": 0.75,\n",
    "      \"preference\": -220,\n",
    "      \"n_clusters\": 2\n",
    "    }\n",
    "  )\n",
    "]\n",
    "\n",
    "default_params = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 20,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6825f-c2e2-46bb-bf31-1941091f14d0",
   "metadata": {},
   "source": [
    "### Define different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3b4c9-051c-45f6-974c-57cdde247923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_clustering_algos(params, **kwargs):\n",
    "\n",
    "    bandwidth = None if not kwargs else kwargs[\"bandwidth\"]\n",
    "    connectivity = None if not kwargs else kwargs[\"connectivity\"]\n",
    "\n",
    "    ms = MeanShift(\n",
    "        bandwidth=bandwidth,\n",
    "        bin_seeding=True\n",
    "    )\n",
    "    two_means = MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"]\n",
    "    )\n",
    "    affinity_propagation = AffinityPropagation(\n",
    "        damping=params[\"damping\"],\n",
    "        preference=params[\"preference\"],\n",
    "        random_state=0\n",
    "    )\n",
    "    ward = AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        linkage=\"ward\",\n",
    "        connectivity=connectivity\n",
    "    )\n",
    "    average_linkage = AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        metric=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        connectivity=connectivity,\n",
    "    )\n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    )\n",
    "    dbscan = DBSCAN(\n",
    "        eps=params[\"eps\"]\n",
    "    )\n",
    "    optics = OPTICS(\n",
    "        min_samples=params[\"min_samples\"],\n",
    "        xi=params[\"xi\"],\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "    )\n",
    "    birch = Birch(\n",
    "        n_clusters=params[\"n_clusters\"]\n",
    "    )\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"],\n",
    "        covariance_type=\"full\"\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"MeanShift\", ms),\n",
    "        (\"MiniBatch\\nKMeans\", two_means),\n",
    "        (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        (\"DBSCAN\", dbscan),\n",
    "        (\"OPTICS\", optics),\n",
    "        (\"BIRCH\", birch),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )\n",
    "\n",
    "    return clustering_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517f0c5-3fb9-4863-ab36-1ce1a524ea52",
   "metadata": {},
   "source": [
    "#### Stolen and reworked from matplotlib's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d1146-7ad1-4bfc-82da-55c30e66ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "nr, nc = len(datasets), len(return_clustering_algos(default_params))\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(4*nc, 4*nr))\n",
    "fig.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "\n",
    "for dataset_i, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_params.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    #X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    extra_params = {\"bandwidth\" : bandwidth, \"connectivity\" : connectivity}\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    clustering_algorithms = return_clustering_algos(params, **extra_params)\n",
    "\n",
    "    for ax_i, (name, algorithm) in enumerate(clustering_algorithms):\n",
    "        # Fit\n",
    "        t0 = time.time()\n",
    "        algorithm.fit(X)\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        ax = axes[dataset_i, ax_i]\n",
    "        ax.axis('off')\n",
    "        if dataset_i == 0:\n",
    "            ax.set_title(name, fontsize=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        ax.scatter(*X.T, color=np.array(colors)[y_pred], s=16)\n",
    "\n",
    "        lim = 1.1 * np.max(np.abs(X))\n",
    "        ax.set_xlim(-lim, lim)\n",
    "        ax.set_ylim(-lim, lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73cad0-7ea9-43df-a4c9-7c481d5926ab",
   "metadata": {},
   "source": [
    "## 3. Classification (#2 in the `problems.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e027278-6b7c-430c-b152-318f24970187",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=100,    # Number of points in the data set\n",
    "    n_features=6,     # Number of features in the data set\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0998e-ca46-4ffb-823c-ed7e5feab786",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3db72-e654-4703-876e-9b3b0db9681f",
   "metadata": {},
   "source": [
    "### Literally the same as regression, but with different methods..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
